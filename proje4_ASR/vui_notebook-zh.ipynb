{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**项目：通过神经网络进行语音识别**\n",
    "\n",
    "## 简介  \n",
    "你完成的管道将接受 __原始音频__ 作为输入，并返回 __所朗读内容__ 的预测转录。    \n",
    "下图总结了完整的管道。\n",
    "\n",
    "<img src=\"images/pipeline.png\">\n",
    "\n",
    "- **第 1 步**是一个 __预处理步骤__ ，<font color=blue>将原始音频转换为两个特征表示之一</font>，这种特征表示常用于 ASR。\n",
    "- **第 2 步**是一个 __声学模型__ ，它<font color=blue>接受音频特征作为输入并返回所有潜在转录的概率分布</font>。了解了<font color=gray>声学建模经常用到的基本神经网络类型</font>后，你将自己调查研究并设计你自己的声学模型！\n",
    "- **第 3 步**：在管道中<font color=blue>通过声学模型的输出返回一个预测转录</font>。  \n",
    "\n",
    "可以根据以下链接浏览此 notebook：\n",
    "- [数据](#thedata)\n",
    "- [**第 1 步**](#step1)：语音识别的声学特征\n",
    "- [**第 2 步**](#step2)：声学建模的深度神经网络\n",
    "    - [模型 0](#model0)：RNN\n",
    "    - [模型 1](#model1)：RNN + TimeDistributed 密集层\n",
    "    - [模型 2](#model2)：CNN + RNN + TimeDistributed 密集层\n",
    "    - [模型 3](#model3)：深度 RNN + TimeDistributed 密集层\n",
    "    - [模型 4](#model4)：双向 RNN + TimeDistributed 密集层\n",
    "    - [模型 5+](#model5)\n",
    "    - [比较模型](#compare)\n",
    "    - [最终模型](#final)\n",
    "- [**第 3 步**](#step3)：获取预测\n",
    "\n",
    "<a id='thedata'></a>\n",
    "## 数据\n",
    "\n",
    "首先，我们研究下将用于训练和评估管道的数据集。  \n",
    "[LibriSpeech](http://www.danielpovey.com/files/2015_icassp_librispeech.pdf) 是一个用英语朗读的大型语料库，用于训练和评估 ASR 模型。该数据集包含 1000 小时的有声读物语音内容。  \n",
    "在此项目中，我们将使用一个 __小型子集__ ，因为更大规模的数据需要更长的训练时间。但是，完成此项目后，如果你想深入了解这方面的知识，建议使用[在线](http://www.openslr.org/12/)提供的更多数据。  \n",
    "\n",
    "在下面的代码单元格中，你将使用 `vis_train_features` 模块可视化训练样本。提供的参数 `index=0` 告诉模块提取训练集中的第一个样本（你可以更改 `index=0` 以指向其他训练样本，但是**请勿**修改单元格中的任何其他代码）。返回的变量为：\n",
    "- `vis_text` - 训练样本的转录文本（标签）。\n",
    "- `vis_raw_audio` - 训练样本的原始音频波形。\n",
    "- `vis_mfcc_feature` - 训练样本的梅尔频率倒谱系数 (MFCC)。\n",
    "- `vis_spectrogram_feature` - 训练样本的频谱图。 \n",
    "- `vis_audio_path` - 训练样本的文件路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error opening '/data/nlpnd_projects/LibriSpeech/dev-clean/1272/141231/1272-141231-0015.wav': System error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c206669ca9ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# extract label and audio features for a single training example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mvis_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvis_raw_audio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvis_mfcc_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvis_spectrogram_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvis_audio_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvis_train_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\workspace_python\\uda_NLP\\proje4_ASR\\data_generator.py\u001b[0m in \u001b[0;36mvis_train_features\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;31m# obtain spectrogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[0maudio_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAudioGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspectrogram\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m     \u001b[0maudio_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_train_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m     \u001b[0mvis_audio_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudio_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_audio_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[0mvis_spectrogram_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudio_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis_audio_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\workspace_python\\uda_NLP\\proje4_ASR\\data_generator.py\u001b[0m in \u001b[0;36mload_train_data\u001b[1;34m(self, desc_file)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_train_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_corpus.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_metadata_from_desc_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesc_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_by_duration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_data_by_duration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\workspace_python\\uda_NLP\\proje4_ASR\\data_generator.py\u001b[0m in \u001b[0;36mfit_train\u001b[1;34m(self, k_samples)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mk_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_audio_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_audio_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m         \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeats_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\workspace_python\\uda_NLP\\proje4_ASR\\data_generator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mk_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_audio_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_audio_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m         \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeats_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\workspace_python\\uda_NLP\\proje4_ASR\\data_generator.py\u001b[0m in \u001b[0;36mfeaturize\u001b[1;34m(self, audio_clip)\u001b[0m\n\u001b[0;32m    238\u001b[0m             return spectrogram_from_file(\n\u001b[0;32m    239\u001b[0m                 \u001b[0maudio_clip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m                 max_freq=self.max_freq)\n\u001b[0m\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_clip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\workspace_python\\uda_NLP\\proje4_ASR\\utils.py\u001b[0m in \u001b[0;36mspectrogram_from_file\u001b[1;34m(filename, step, window, max_freq, eps)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0meps\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSmall\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mto\u001b[0m \u001b[0mensure\u001b[0m \u001b[0mnumerical\u001b[0m \u001b[0mstability\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mfor\u001b[0m \u001b[0mln\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0msoundfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msound_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msound_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0msample_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msound_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\setup_room\\python\\lib\\site-packages\\soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    625\u001b[0m         self._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    626\u001b[0m                                          format, subtype, endian)\n\u001b[1;32m--> 627\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'r+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m             \u001b[1;31m# Move write position to 0 (like in Python file objects)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\setup_room\\python\\lib\\site-packages\\soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid file: {0!r}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m         _error_check(_snd.sf_error(file_ptr),\n\u001b[1;32m-> 1182\u001b[1;33m                      \"Error opening {0!r}: \".format(self.name))\n\u001b[0m\u001b[0;32m   1183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[1;31m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\setup_room\\python\\lib\\site-packages\\soundfile.py\u001b[0m in \u001b[0;36m_error_check\u001b[1;34m(err, prefix)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msf_error_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1355\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'replace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error opening '/data/nlpnd_projects/LibriSpeech/dev-clean/1272/141231/1272-141231-0015.wav': System error."
     ]
    }
   ],
   "source": [
    "from data_generator import vis_train_features\n",
    "\n",
    "# extract label and audio features for a single training example\n",
    "vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path = vis_train_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码单元格会可视化所选样本的音频波形，以及对应的转录。你还可以在 notebook 中实验音频！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import vis_train_features, plot_raw_audio\n",
    "\n",
    "from IPython.display import Markdown, display                       # display Markdown in IPython\n",
    "from IPython.display import Audio                                   # display audio\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# plot audio signal\n",
    "plot_raw_audio(vis_raw_audio)\n",
    "\n",
    "# print length of audio signal\n",
    "display(Markdown('**Shape of Audio Signal** : ' + str(vis_raw_audio.shape)))\n",
    "\n",
    "# print transcript corresponding to audio clip\n",
    "display(Markdown('**Transcript** : ' + str(vis_text)))\n",
    "\n",
    "# play the audio file\n",
    "Audio(vis_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
    "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## 第 1 步：语音识别的声学特征\n",
    "\n",
    "- 建议阅读: [研究论文](https://pdfs.semanticscholar.org/a566/cd4a8623d661a4931814d9dffc72ecbf63c4.pdf)。\n",
    "\n",
    "首先会执行预处理步骤的代码，<font color=blue>将原始音频转换为一种特征表示法</font>，经证明这种表示法能够使 ASR 模型成功完成任务。你的声学模型将接受该特征表示法作为输入。\n",
    "\n",
    "在此项目中，你将探索 __两种潜在特征表示法__ 。\n",
    "- 第一种音频特征表示法是[声谱图](https://www.youtube.com/watch?v=_FatxGN3vAM)。 \n",
    "- 第二种音频特征表示法是 [MFCC](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)。\n",
    "\n",
    "### 声谱图\n",
    "- 为了完成此项目，你**不**需要深入了解声谱图是如何计算的；\n",
    "\n",
    "<font color=gray>但是如果你好奇的话，计算声谱图的代码其实源自[此代码库](https://github.com/baidu-research/ba-dls-deepspeech)。实现代码位于代码库中的 `utils.py` 文件里。</font>\n",
    "\n",
    "我们提供的代码返回的声谱图是 __二维张量__ ，第一个维度（ _垂直维度_ ）表示 __时间__ ，第二个维度（ _水平维度_ ）表示 __频率__ 。  \n",
    "要提高算法收敛速度，我们还 __标准化了声谱图__ 。（你可以在下面的可视化图形中快速看到这一点，可以发现均值在零左右，张量中的大多数条目假设值接近零。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vis_spectrogram_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9858c6aa7e78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# plot normalized spectrogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplot_spectrogram_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis_spectrogram_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# print shape of spectrogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vis_spectrogram_feature' is not defined"
     ]
    }
   ],
   "source": [
    "from data_generator import plot_spectrogram_feature\n",
    "\n",
    "# plot normalized spectrogram\n",
    "plot_spectrogram_feature(vis_spectrogram_feature)\n",
    "\n",
    "# print shape of spectrogram\n",
    "display(Markdown('**Shape of Spectrogram** : ' + str(vis_spectrogram_feature.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梅尔频率倒谱系数 (MFCC)\n",
    "\n",
    "- 建议阅读 `python_speech_features` Python 软件包的[文档](https://github.com/jameslyons/python_speech_features)。\n",
    "和声谱图特征一样，提供的代码标准化了 MFCC。\n",
    "\n",
    "MFCC 特征的主要原理和声谱图特征一样：在每个时间窗口，MFCC 特征都会生成一个特征向量，表示 __该窗口中的声音特性__ 。注意，MFCC 特征的 __维度__ 比声谱图特征的低了很多，有助于声学模型避免过拟合训练数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import plot_mfcc_feature\n",
    "\n",
    "# plot normalized MFCC\n",
    "plot_mfcc_feature(vis_mfcc_feature)\n",
    "# print shape of MFCC\n",
    "display(Markdown('**Shape of MFCC** : ' + str(vis_mfcc_feature.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在构建管道时，你将能够选择使用声谱图还是 MFCC 特征。如果你要查看利用 MFCC 和/或声谱图的不同实现，请访问以下链接：\n",
    "- 此[代码库](https://github.com/baidu-research/ba-dls-deepspeech)使用声谱图。\n",
    "- 此[代码库](https://github.com/mozilla/DeepSpeech)使用 MFCC。\n",
    "- 此[代码库](https://github.com/buriburisuri/speech-to-text-wavenet)也使用 MFCC。\n",
    "- 此[代码库](https://github.com/pannous/tensorflow-speech-recognition/blob/master/speech_data.py)用原始音频、声谱图和 MFCC 作为特征进行实验。\n",
    "\n",
    "<a id='step2'></a>\n",
    "## 第 2 步：进行声学建模的深度神经网络\n",
    "\n",
    "在此部分，你将尝试用 __各种神经网络架构__ 进行声学建模。 \n",
    "\n",
    "首先，你将训练五个相对简单的架构。如果想进一步实验，需要在**模型 5+** 标题下方创建和训练更多模型。\n",
    "\n",
    "所有模型都将在 `sample_models.py` 文件中指定。导入 `sample_models` 模块后，你将在 notebook 中训练你的架构。\n",
    "\n",
    "实验了五个简单架构后，你将有机会比较它们的性能。你将根据你的发现，<font color=blue>构建一个旨在比所有其他模型性能都要强的深度架构。</font>\n",
    "\n",
    "即，假设在训练**模型 1** 之后，你决定休息一下。那么在训练**模型 2** 之前，你不需要重新执行 notebook 中所有之前的代码单元格。在转到**模型 2** 对应的代码单元格之前，只需重新执行下面的代码单元格，  \n",
    "标记为 **`RUN THIS CODE CELL IF YOU ARE RESUMING THE NOTEBOOK AFTER A BREAK`**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# RUN THIS CODE CELL IF YOU ARE RESUMING THE NOTEBOOK AFTER A BREAK #\n",
    "#####################################################################\n",
    "\n",
    "# allocate 50% of GPU memory (if you like, feel free to change this)\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# watch for any changes in the sample_models module, and reload it automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import NN architectures for speech recognition\n",
    "# simple_rnn_model, \n",
    "from sample_models import *\n",
    "# import function for training acoustic model\n",
    "from train_utils import train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__train_mode__\n",
    "___\n",
    "- 要训练架构，你将在 `train_utils` 模块中使用 `train_model` 函数；已经在上述代码单元格中导入该函数。\n",
    "\n",
    "- `train_model` 函数**必须**传入三个参数：\n",
    "    - `input_to_softmax` - 一个 Keras 模型实例。\n",
    "    - `pickle_path` - 将保存损失记录的 pickle 文件的名称。\n",
    "    - `save_model_path` - 将保存模型的 HDF5 文件的名称。\n",
    "\n",
    "- 如果我们已经提供 `input_to_softmax`、`pickle_path`  和 `save_model_path` 的值，请**不要**修改这些值。\n",
    "\n",
    "- 有几个**可选**参数使你能够更好地控制训练流程。你可以为这些参数提供你自己的值，但是并非必须这么做。\n",
    "    - `minibatch_size` - 训练模型时生成的迷你批次的大小（默认为 `20`）。\n",
    "    - `spectrogram` - 布尔值，表示是使用声谱图 (`True`) 还是 MFCC (`False`) 特征训练模型（默认为：`True`）。\n",
    "    - `mfcc_dim` - 生成 MFCC 特征时使用的特征维度大小（默认为：`13`）。\n",
    "    - `optimizer` - 训练模型所使用的 Keras 优化器（默认为：`SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)`)。 \n",
    "    - `epochs` - 训练模型所使用的周期次数（默认为：`20`）。如果你选择修改此参数，确保*至少为* 20。\n",
    "    - `verbose` - 控制 `model.fit_generator` 方法中的训练输出的详细程度（默认为：`1`）。\n",
    "    - `sort_by_duration` - 布尔值，表示是否在第一个周期开始前按照时长对训练集和测试集进行排序（升序）（默认为：`False`）。\n",
    "\n",
    "- `train_model` 函数默认使用声谱图特征；  \n",
    "    - 如果你决定使用 __声谱图特征__ ，注意 `simple_rnn_model` 中的声学模型应该设置 `input_dim=161`  \n",
    "    - 如果你选择使用 __MFCC 特征__ ，声学模型应该设置 `input_dim=13`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SimpleRNN&RNN/GRU__   \n",
    "___\n",
    "<font color=blue>在提供的 RNN 中，我们选择使用 `GRU` 单元。如果你想尝试 `LSTM` 或 `SimpleRNN` 单元，可以在此 Notebook 中尝试</font>  \n",
    "你可以在 `simple_rnn_model` 中将 `GRU` 单元更改为 `SimpleRNN` 单元，你可能会发现损失函数很快变得未定义  (`nan`) - 强烈建议你自己检查这一点！这是由[梯度激增问题](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)导致的。   \n",
    "我们已经在优化器中实现了[梯度裁剪](https://arxiv.org/pdf/1211.5063.pdf)方法，帮助你避免此问题。\n",
    "\n",
    "<font color=red>注意</font>：如果你发现在以下任何模型中，你的梯度激增了，你可以\n",
    "- 尝试 __梯度裁剪__ （优化器中的 `clipnorm` 参数）\n",
    "- 或将任何 `SimpleRNN` 单元格更改为 `LSTM` 或 `GRU` 单元格。\n",
    "- 或尝试重启核，以重启训练流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model0'></a>\n",
    "### 模型 0：RNN\n",
    "\n",
    "因为 RNN 在 __序列数据建模方面__ 的效果不错，因此你将使用的第一个声学模型是 RNN。  \n",
    "- 如下图所示，RNN 将用 __音频特征的时间序列__ 作为输入。\n",
    "\n",
    "<img src=\"images/simple_rnn.png\" width=\"50%\">\n",
    "\n",
    "在每个时间步，说话者会发出 __28 个潜在字符中的某个字符__ ，包括<font color=green>英语字母表中的 26 个字母，以及空格 (\" \") 和撇号 (')</font>\n",
    "    \n",
    "RNN 在每个时间步的输出是 __一个概率向量__ ，有 __29 个条目__ ，第 $i$- 个条目表示 __在时间序列中发出第 $i$ 个字符的概率__ 。（额外的第 29 个字符是一个“空”字符，用于填充批次中长度不均匀的训练样本。）\n",
    "<font color=gray>如果你要了解字符是如何映射到概率向量中的索引的，请参阅代码库中的 `char_map.py` 文件。</font>\n",
    "\n",
    "- 下图是 RNN 的 __等效展开模式__ ，更详细地显示了输出层。\n",
    "\n",
    "<img src=\"images/simple_rnn_unrolled.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = simple_rnn_model(input_dim=161) # change to 13 if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如在课程中所讲解的，你将使用 [CTC 损失](http://www.cs.toronto.edu/~graves/icml_2006.pdf)条件训练声学模型。  \n",
    "Keras 中的自定义损失函数比较复杂，因此我们已经为你实现了 CTC 损失函数，使你能够专注于尽量尝试更多的深度学习架构 :)\n",
    "\n",
    "<font color=gray>如果你想了解实现细节，请参阅代码库中的 `train_utils.py` 文件中的 `add_ctc_loss` 函数</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_0, \n",
    "            pickle_path='model_0.pickle', \n",
    "            save_model_path='model_0.h5',\n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model1'></a>\n",
    "### （实现）模型 1：RNN + TimeDistributed 密集层\n",
    "\n",
    "请参阅关于 [TimeDistributed](https://keras.io/layers/wrappers/) 封装器和 [BatchNormalization](https://keras.io/layers/normalization/) 层级的 Keras 文档。   \n",
    "对于下个架构，你将向递归层添加 __[批次标准化](https://arxiv.org/pdf/1510.01378.pdf)方法__ ，以 _缩短训练时间_ 。 __`TimeDistributed` 层级__ 将用于 _查找数据集中的更复杂模式_ 。  \n",
    "- 该架构的合拢快照如下图所示。\n",
    "\n",
    "<img src=\"images/rnn_model.png\" width=\"60%\">\n",
    "\n",
    "- 下图是一个 __等效的 RNN 展开__ 图，更详细地显示了 __(`TimeDistrbuted`) 密集层和输出层__ 。 \n",
    "\n",
    "<img src=\"images/rnn_model_unrolled.png\" width=\"60%\">\n",
    "\n",
    "<font color=red>请自己研究并完成 `sample_models.py` 文件中的 `rnn_model` 函数</font>    \n",
    "该函数应该指定一个满足以下要求的架构：\n",
    "\n",
    "- 该神经网络的\n",
    "    - 第一层应该是一个 RNN（`SimpleRNN`、`LSTM` 或 `GRU`），输入为音频特征时间序列。\n",
    "    \\* 我们已经为你添加 `GRU` 单元，但是你可以将 `GRU` 更改为 `SimpleRNN` 或 `LSTM`！\n",
    "- 虽然 `simple_rnn_model` 中的架构将 RNN 输出当做模型的最后层级，你将使用 RNN 的输出作为隐藏层。请使用 `TimeDistributed` 向 RNN 输出中的每个时间步应用一个 `Dense` 层级。确保每个 `Dense` 层级具有 `output_dim` 单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(input_dim, units, activation, output_dim=29):\n",
    "    \"\"\" Build a recurrent network for speech \n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # Add recurrent layer\n",
    "    simp_rnn = GRU(units,\n",
    "                   activation=activation,\n",
    "                   return_sequences=True,\n",
    "                   implementation=2,\n",
    "                   name='rnn')(input_data)\n",
    "    # TODO: Add batch normalization \n",
    "    bn_rnn = BatchNormalization(name='bn_rnn')(simp_rnn)\n",
    "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim),\n",
    "                                 name='time_dense')(bn_rnn)\n",
    "    # Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = rnn_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
    "                    units=200,\n",
    "                    activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请执行以下代码单元格，以训练你在 `input_to_softmax` 中指定的神经网络。模型训练完毕后，[保存](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)在 HDF5 文件 `model_1.h5` 中。损失记录[保存](https://wiki.python.org/moin/UsingPickle)在 `model_1.pickle` 中。在调用 `train_model` 函数时，你可以调整任何可选参数，但是并非必须这么做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_1, \n",
    "            pickle_path='model_1.pickle', \n",
    "            save_model_path='model_1.h5',\n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model2'></a>\n",
    "### （实现）模型 2：CNN + RNN + TimeDistributed 密集层\n",
    "\n",
    "`cnn_rnn_model` 中的架构通过引入 __[一维卷积层]__ (https://keras.io/layers/convolutional/#conv1d)，增加了一层复杂性。 \n",
    "\n",
    "<img src=\"images/cnn_rnn_model.png\" width=\"100%\">\n",
    "\n",
    "此层级包含很多在调用 `cnn_rnn_model` 模块时可以调整（可选操作）的参数。我们提供了示例起始参数，如果你选择使用 __声谱图音频特征__ ，可能会发现这些参数很有用。 \n",
    "\n",
    "如果你想使用 MFCC 特征，则需要调整这些参数。注意，对于 `conv_border_mode` 参数来说，当前架构 <font color=blue>只支持 `'same'` 或 `'valid'` 值。</font>\n",
    "\n",
    "在调整参数时，请注意避免选择导致 <font color=red>卷积层过小</font>的设置。如果<font color=red>CNN 层级的时序长度比转录文本标签的长度短，代码将抛出错误。</font>\n",
    "\n",
    "请向递归层添加批次标准化方法，并提供和之前一样的 `TimeDistributed` 层级。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_rnn_model(input_dim, filters, kernel_size, conv_stride,\n",
    "    conv_border_mode, units, output_dim=29):\n",
    "    \"\"\" Build a recurrent + convolutional network for speech \n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input',\n",
    "                       shape=(None, input_dim))\n",
    "    # Add convolutional layer\n",
    "    conv_1d = Conv1D(filters, kernel_size, \n",
    "                     strides=conv_stride, \n",
    "                     padding=conv_border_mode,\n",
    "                     activation='relu',\n",
    "                     name='conv1d')(input_data)\n",
    "    # Add batch normalization\n",
    "    bn_cnn = BatchNormalization(name='bn_conv_1d')(conv_1d)\n",
    "    # Add a recurrent layer\n",
    "    simp_rnn = SimpleRNN(units,\n",
    "                         activation='relu',\n",
    "                         return_sequences=True,\n",
    "                         implementation=2,\n",
    "                         name='rnn')(bn_cnn)\n",
    "    # TODO: Add batch normalization\n",
    "    bn_rnn = BatchNormalization(name='bn_rnn')(simp_rnn)\n",
    "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    # Add softmax activation layer\n",
    "    y_pred = Activation('softmax',\n",
    "                        name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data,\n",
    "                  outputs=y_pred)\n",
    "    model.output_length = lambda x: cnn_output_length(\n",
    "        x, kernel_size, conv_border_mode, conv_stride)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = cnn_rnn_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
    "                        filters=200,\n",
    "                        kernel_size=11, \n",
    "                        conv_stride=2,\n",
    "                        conv_border_mode='valid',\n",
    "                        units=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请执行以下代码单元格，以训练你在 `input_to_softmax` 中指定的神经网络。   \n",
    "模型训练完毕后，[保存](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)在 __HDF5__ 文件 `model_2.h5` 中。损失记录[保存](https://wiki.python.org/moin/UsingPickle)在 `model_2.pickle` 中。    \n",
    "在调用 `train_model` 函数时，你可以调整任何可选参数，但是并非必须这么做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_2, \n",
    "            pickle_path='model_2.pickle', \n",
    "            save_model_path='model_2.h5', \n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model3'></a>\n",
    "### （实现）模型 3：深度 RNN + TimeDistributed 密集层\n",
    "\n",
    "请查看 `rnn_model` 中的代码，该模型使用了一个递归层。现在，请在 `deep_rnn_model` 中指定 __一个利用多个 (`recur_layers`) 递归层__ 的架构。    \n",
    "下图显示了如果 `recur_layers=2` 应该返回的架构。在此图中，第一个递归层的输出序列用作下个递归层的输入。\n",
    "\n",
    "<img src=\"images/deep_rnn_model.png\" width=\"80%\">\n",
    "\n",
    "- 你可以将提供的 `units` 值更改为你认为效果最佳的值。  \n",
    "- 你可以更改 `recur_layers` 的值，只要最终值大于 1 即可。   \n",
    "（为了快速检查你是否在 `deep_rnn_model` 中正确实现了额外功能，如果 `recur_layers=1`，确保你在此文件中指定的架构与 `rnn_model` 完全一样。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_rnn_model(input_dim, units, recur_layers, output_dim=29):\n",
    "    \"\"\" Build a deep recurrent network for speech \n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # TODO: Add recurrent layers, each with batch normalization\n",
    "    for i in range(recur_layers):\n",
    "        if i is 0:\n",
    "            gru = GRU(units,\n",
    "                   activation='relu',\n",
    "                   return_sequences=True,\n",
    "                   implementation=2,\n",
    "                   name='rnn')(input_data)\n",
    "            \n",
    "        else:\n",
    "            gru = GRU(units,\n",
    "                   activation='relu',\n",
    "                   return_sequences=True,\n",
    "                   implementation=2,\n",
    "                   name='rnn')(bn_cnn)\n",
    "        bn_cnn = BatchNormalization(name='bn_conv_1d')(gru)\n",
    "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_cnn)\n",
    "    # Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = deep_rnn_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
    "                         units=200,\n",
    "                         recur_layers=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请执行以下代码单元格，以训练你在 `input_to_softmax` 中指定的神经网络。模型训练完毕后，[保存](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)在 HDF5 文件 `model_3.h5` 中。损失记录[保存](https://wiki.python.org/moin/UsingPickle)在 `model_3.pickle` 中。在调用 `train_model` 函数时，你可以调整任何可选参数，但是并非必须这么做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_3, \n",
    "            pickle_path='model_3.pickle', \n",
    "            save_model_path='model_3.h5', \n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model4'></a>\n",
    "### （实现）模型 4：双向 RNN + TimeDistributed 密集层\n",
    "\n",
    "阅读[双向](https://keras.io/layers/wrappers/)封装器的 Keras 文档。对于下个架构，你将指定一个在 (`TimeDistributed`) 密集层之前使用单个双向 RNN 层级的架构。[此论文](http://www.cs.toronto.edu/~hinton/absps/DRNN_speech.pdf)详细解释了双向 RNN 的优势。\n",
    "> 传统 RNN 的一个缺点是它们只能利用之前的语境。在语音识别中，整个话语都会一次性转录，因此也需要利用后续语境。双向 RNN (BRNN) 可以满足此需求，它使用两个单独的隐藏层朝着两个方向处理数据，然后，两个隐藏层的结果都传入相同的输出层。\n",
    "\n",
    "<img src=\"images/bidirectional_rnn_model.png\" width=\"80%\">\n",
    "\n",
    "在运行以下代码单元格之前，你必须完成 `sample_models.py` 中的 `bidirectional_rnn_model` 函数。你可以使用 `SimpleRNN`、`LSTM` 或 `GRU` 单元。在指定 `Bidirectional` 封装器时，请使用 `merge_mode='concat'`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_rnn_model(input_dim, units, output_dim=29):\n",
    "    \"\"\" Build a bidirectional recurrent network for speech\n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # TODO: Add bidirectional recurrent layer\n",
    "    bidir_rnn = Bidirectional(GRU(units, return_sequences=True))(input_data)\n",
    "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
    "    time_dense = TimeDistributed(Dense(input_dim))(bidir_rnn)\n",
    "    # Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, None, 161)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 400)         434400    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 161)         64561     \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 161)         0         \n",
      "=================================================================\n",
      "Total params: 498,961\n",
      "Trainable params: 498,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_4 = bidirectional_rnn_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
    "                                  units=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请执行以下代码单元格，以训练你在 `input_to_softmax` 中指定的神经网络。模型训练完毕后，[保存](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)在 HDF5 文件 `model_4.h5` 中。损失记录[保存](https://wiki.python.org/moin/UsingPickle)在 `model_4.pickle` 中。在调用 `train_model` 函数时，你可以调整任何可选参数，但是并非必须这么做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_4, \n",
    "            pickle_path='model_4.pickle', \n",
    "            save_model_path='model_4.h5', \n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model5'></a>\n",
    "### （可选实现）模型 5+\n",
    "\n",
    "如果你想尝试上述架构之外的其他架构，请使用以下代码单元格。请继续按照相同的格式保存模型；对于第 $i$ 个示例模型，请将损失保存到 **`model_i.pickle`**，将训练的模型保存到 **`model_i.h5`**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) TODO: Try out some more models!\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='compare'></a>\n",
    "### 比较模型\n",
    "\n",
    "执行以下代码单元格，评估草稿深度学习模型的性能。我们绘制了每个模型的训练和验证损失图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style(style='white')\n",
    "\n",
    "# obtain the paths for the saved model history\n",
    "all_pickles = sorted(glob(\"results/*.pickle\"))\n",
    "# extract the name of each model\n",
    "model_names = [item[8:-7] for item in all_pickles]\n",
    "# extract the loss history for each model\n",
    "valid_loss = [pickle.load( open( i, \"rb\" ) )['val_loss'] for i in all_pickles]\n",
    "train_loss = [pickle.load( open( i, \"rb\" ) )['loss'] for i in all_pickles]\n",
    "# save the number of epochs used to train each model\n",
    "num_epochs = [len(valid_loss[i]) for i in range(len(valid_loss))]\n",
    "\n",
    "fig = plt.figure(figsize=(16,5))\n",
    "\n",
    "# plot the training loss vs. epoch for each model\n",
    "ax1 = fig.add_subplot(121)\n",
    "for i in range(len(all_pickles)):\n",
    "    ax1.plot(np.linspace(1, num_epochs[i], num_epochs[i]), \n",
    "            train_loss[i], label=model_names[i])\n",
    "# clean up the plot\n",
    "ax1.legend()  \n",
    "ax1.set_xlim([1, max(num_epochs)])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "\n",
    "# plot the validation loss vs. epoch for each model\n",
    "ax2 = fig.add_subplot(122)\n",
    "for i in range(len(all_pickles)):\n",
    "    ax2.plot(np.linspace(1, num_epochs[i], num_epochs[i]), \n",
    "            valid_loss[i], label=model_names[i])\n",
    "# clean up the plot\n",
    "ax2.legend()  \n",
    "ax2.set_xlim([1, max(num_epochs)])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__问题 1：__ 利用上图分析每个实验架构的性能。哪个架构的性能最好？请解释下你为何认为某些模型的性能比其他模型好。\n",
    "\n",
    "__答案：__ ：<font color=green>模型3（DRNNs）更好。因为训练收敛快，训练epochs增加时，过拟合状况也没有出现。</font>\n",
    "\n",
    "<a id='final'></a>\n",
    "### （实现）最终模型\n",
    "\n",
    "虽然最终声学模型不一定要与上面探索的任何架构完全一样，但是你可以仅将上面探索的层级添加到更深的架构中。\n",
    "**并非**必须包含在 notebook 中没有探索过的新层级类型      \n",
    "\n",
    "但是，如果你想了解 __如何添加更多种类的层级__ ，请参阅以下内容，了解如何向模型中添加其他可选扩展功能：\n",
    "\n",
    "- 如果你发现模型过拟合训练数据集，请考虑添加**丢弃**(Dropout)！要向[递归层](https://faroit.github.io/keras-docs/1.0.2/layers/recurrent/)中添加丢弃，请注意 `dropout_W` 和 `dropout_U` 参数。此[论文](http://arxiv.org/abs/1512.05287)也提供了一些有趣的理论知识。\n",
    "\n",
    "- 如果你选择在模型中添加卷积层，使用**扩张卷积**可能会获得更好的结果。如果选择使用扩张卷积，确保能够在 `model.output_length` lambda 函数中准确计算声学模型的输出长度。你可以在 Google 的 [WaveNet 论文](https://arxiv.org/abs/1609.03499)中详细了解扩张卷积。要查看利用扩张卷积的语音转文字系统示例，请参阅此 GitHub [代码库](https://github.com/buriburisuri/speech-to-text-wavenet)。你可以[在 Keras](https://keras.io/layers/convolutional/) 中使用扩张卷积，但是在指定卷积层时需要特别注意 `padding` 参数。\n",
    "- 如果你的模型利用了卷积层，为何不添加**最大池化**层呢？请参阅[此论文](https://arxiv.org/pdf/1701.02720.pdf)，了解在声学模型中利用最大池化层的示例架构。\n",
    "- 到目前为止，你尝试了单个双向 RNN 层级。可以考虑堆叠双向层级，生成[深度双向 RNN](https://www.cs.toronto.edu/~graves/asru_2013.pdf)！\n",
    "\n",
    "你在此代码库中指定的所有模型应该定义属性 `output_length`。此属性是一个 lambda 函数，可以将输入声学特征的（时序）长度映射到输出 softmax 层。此函数可以用于计算 CTC 损失；要查看这一点，请查看 `train_utils.py` 中的 `add_ctc_loss` 函数。要查看在代码中的什么位置定义模型的 `output_length` 属性，请查看 `sample_models.py` 文件。你将在大多数模型中注意到以下这行代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_length = lambda x: x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包含卷积层的声学模型 (`cnn_rnn_model`) 具有稍微不同的代码行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_length = lambda x: cnn_output_length(\n",
    "        x, kernel_size, conv_border_mode, conv_stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于完全使用递归层的模型，该 lambda 函数是一个恒等函数，因为递归层不会修改输入张量的（时序）长度。但是，卷积层更复杂，需要使用特殊函数（`sample_models.py` 中的 `cnn_output_length`） 确定输出的时序长度。\n",
    "\n",
    "在运行以下代码单元格之前，你需要将 `output_length` 属性添加到最终模型中。你可以使用 `cnn_output_length` 函数，如果它适合你的模型的话。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_output_length(input_length, filter_size, border_mode, stride,\n",
    "                       dilation=1):\n",
    "    \"\"\" Compute the length of the output sequence after 1D convolution along\n",
    "        time. Note that this function is in line with the function used in\n",
    "        Convolution1D class from Keras.\n",
    "    Params:\n",
    "        input_length (int): Length of the input sequence.\n",
    "        filter_size (int): Width of the convolution kernel.\n",
    "        border_mode (str): Only support `same` or `valid`.\n",
    "        stride (int): Stride size used in 1D convolution.\n",
    "        dilation (int)\n",
    "    \"\"\"\n",
    "    if input_length is None:\n",
    "        return None\n",
    "    assert border_mode in {'same', 'valid'}\n",
    "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "    if border_mode == 'same':\n",
    "        output_length = input_length\n",
    "    elif border_mode == 'valid':\n",
    "        output_length = input_length - dilated_filter_size + 1\n",
    "    return (output_length + stride - 1) // stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(input_dim, filters, kernel_size, conv_stride,\n",
    "    conv_border_mode, units, recur_layers, output_dim=29):\n",
    "    \"\"\" Build a deep network for speech\n",
    "    - BNNs + CRNNs + DRNNs + BRNNs\n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # TODO: Specify the layers in your network\n",
    "    # BRNNs\n",
    "    bidir_rnn = Bidirectional(GRU(units, return_sequences=True))(input_data)\n",
    "\n",
    "    # CRNNs\n",
    "    conv_1d = Conv1D(filters, kernel_size,\n",
    "                     strides=conv_stride,\n",
    "                     padding=conv_border_mode,\n",
    "                     activation='relu',\n",
    "                     name='conv1d')(bidir_rnn)\n",
    "\n",
    "    # DRNNs\n",
    "    for i in range(recur_layers):\n",
    "        if i is 0:\n",
    "            gru = GRU(units,\n",
    "                      activation='relu',\n",
    "                      return_sequences=True,\n",
    "                      implementation=2,\n",
    "                      name='rnn%d'%i)(conv_1d)\n",
    "\n",
    "        else:\n",
    "            gru = GRU(units,\n",
    "                      activation='relu',\n",
    "                      return_sequences=True,\n",
    "                      implementation=2,\n",
    "                      name='rnn%d'%i)(bn_cnn)\n",
    "        bn_cnn = BatchNormalization(name='bn_conv_1d%d'%i)(gru)\n",
    "\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_cnn)\n",
    "\n",
    "    # TODO: Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    # TODO: Specify model.output_length\n",
    "    model.output_length = lambda x: cnn_output_length(\n",
    "        x, kernel_size, conv_border_mode, conv_stride)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e8cd6ee71728>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                         \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                         \u001b[0mrecur_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                         output_dim=29)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-a56e823b33bd>\u001b[0m in \u001b[0;36mfinal_model\u001b[1;34m(input_dim, filters, kernel_size, conv_stride, conv_border_mode, units, recur_layers, output_dim)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Main acoustic input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0minput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'the_input'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# TODO: Specify the layers in your network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# BRNNs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "# specify the model\n",
    "## (input_dim, filters, kernel_size, conv_stride,\n",
    "##  conv_border_mode, units, recur_layers, output_dim=29)\n",
    "\n",
    "model_end = final_model(input_dim=161, \n",
    "                        filters=200, \n",
    "                        kernel_size=11, \n",
    "                        conv_stride=2,\n",
    "                        conv_border_mode='valid', \n",
    "                        units=200, \n",
    "                        recur_layers=2, \n",
    "                        output_dim=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请执行以下代码单元格，以训练你在 `input_to_softmax` 中指定的神经网络。模型训练完毕后，[保存](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)在 HDF5 文件 `model_end.h5` 中。损失记录[保存](https://wiki.python.org/moin/UsingPickle)在 `model_end.pickle` 中。在调用 `train_model` 函数时，你可以调整任何可选参数，但是并非必须这么做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_end, \n",
    "            pickle_path='model_end.pickle', \n",
    "            save_model_path='model_end.h5', \n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__问题 2：__描述你的最终模型架构和每一步的推理过程。  \n",
    "\n",
    "__答案：__\n",
    "\n",
    "<a id='step3'></a>\n",
    "## 第 3 步：获取预测\n",
    "\n",
    "我们为你编写了一个函数，用于解码声学模型的预测。要使用该函数，请执行以下代码单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_generator import AudioGenerator\n",
    "from keras import backend as K\n",
    "from utils import int_sequence_to_text\n",
    "from IPython.display import Audio\n",
    "\n",
    "def get_predictions(index, partition, input_to_softmax, model_path):\n",
    "    \"\"\" Print a model's decoded predictions\n",
    "    Params:\n",
    "        index (int): The example you would like to visualize\n",
    "        partition (str): One of 'train' or 'validation'\n",
    "        input_to_softmax (Model): The acoustic model\n",
    "        model_path (str): Path to saved acoustic model's weights\n",
    "    \"\"\"\n",
    "    # load the train and test data\n",
    "    data_gen = AudioGenerator()\n",
    "    data_gen.load_train_data()\n",
    "    data_gen.load_validation_data()\n",
    "    \n",
    "    # obtain the true transcription and the audio features \n",
    "    if partition == 'validation':\n",
    "        transcr = data_gen.valid_texts[index]\n",
    "        audio_path = data_gen.valid_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    elif partition == 'train':\n",
    "        transcr = data_gen.train_texts[index]\n",
    "        audio_path = data_gen.train_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    else:\n",
    "        raise Exception('Invalid partition!  Must be \"train\" or \"validation\"')\n",
    "        \n",
    "    # obtain and decode the acoustic model's predictions\n",
    "    input_to_softmax.load_weights(model_path)\n",
    "    prediction = input_to_softmax.predict(np.expand_dims(data_point, axis=0))\n",
    "    output_length = [input_to_softmax.output_length(data_point.shape[0])] \n",
    "    pred_ints = (K.eval(K.ctc_decode(\n",
    "                prediction, output_length)[0][0])+1).flatten().tolist()\n",
    "    \n",
    "    # play the audio file, and display the true and predicted transcriptions\n",
    "    print('-'*80)\n",
    "    Audio(audio_path)\n",
    "    print('True transcription:\\n' + '\\n' + transcr)\n",
    "    print('-'*80)\n",
    "    print('Predicted transcription:\\n' + '\\n' + ''.join(int_sequence_to_text(pred_ints)))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请通过以下代码单元格获取最终模型对训练数据集中第一个样本预测的转录结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(index=0, \n",
    "                partition='train',\n",
    "                input_to_softmax=final_model(), \n",
    "                model_path='model_end.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过下个代码单元格可视化模型对验证数据集中第一个样本的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(index=0, \n",
    "                partition='validation',\n",
    "                input_to_softmax=final_model(), \n",
    "                model_path='model_end.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改善解码器结果的一种标准方式是添加语言模型。在此 notebook 中，我们不要求这么做，但是你可以将此过程当做_可选拓展练习_。\n",
    "\n",
    "如果你想创建转录效果更好的模型，建议下载[更多数据](http://www.openslr.org/12/)并训练更大更深的模型。但是注意，模型可能需要更长的训练时间。例如，在单个 GPU 上训练这个[先进的](https://arxiv.org/pdf/1512.02595v1.pdf)模型需要 3-6 周时间！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "312px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
