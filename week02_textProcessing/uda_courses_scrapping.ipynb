{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本清理（从网页抓取文字）\n",
    "\n",
    "```\n",
    "<a _ngcontent-iridium-cn-c20=\"\" href=\"/course/ai-product-manager-nanodegree--nd088-cn\"> AI 产品经理 (英)</a>\n",
    "<span _ngcontent-iridium-cn-c20=\"\" class=\"ng-star-inserted\">驱动 AI 产品商业化落地，用 AI 为业务赋能，成为炙手可热稀缺人才。</span>\n",
    "```\n",
    "\n",
    "```python\n",
    "url = 'https://cn.udacity.com/courses/all'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://cn.udacity.com/courses/all'\n",
    "\n",
    "req = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(req.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演示\n",
    "- selecor选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div _ngcontent-sc227=\"\" class=\"course-summary-card row row-gap-medium ng-star-inserted\"><div _ngcontent-sc227=\"\" class=\"col-sm-3\"><!-- --><img _ngcontent-sc227=\"\" alt=\"AI 产品经理 (英)\" class=\"course-thumb img-responsive img-bordered center-block ng-star-inserted\" height=\"170\" src=\"https://s3.cn-north-1.amazonaws.com.cn/static-assets/degrees/nd088-cn/nd-card.jpg\" width=\"290\"/><!-- --><!-- --></div><div _ngcontent-sc227=\"\" class=\"col-sm-9 course-text\"><div _ngcontent-sc227=\"\" class=\"row\"><div _ngcontent-sc227=\"\" class=\"col-sm-8\"><h3 _ngcontent-sc227=\"\" class=\"h-slim\"><a _ngcontent-sc227=\"\" href=\"/course/ai-product-manager-nanodegree--nd088-cn\"> AI 产品经理 (英)</a><span _ngcontent-sc227=\"\" class=\"badges\"><!-- --></span></h3></div><div _ngcontent-sc227=\"\" class=\"col-sm-4 hidden-xs\"><!-- --><span _ngcontent-sc227=\"\" class=\"caption text-right ng-star-inserted\"><span _ngcontent-sc227=\"\" class=\"icon-level course-info-bar-icon icon-level-beginner\" classname=\"icon-level course-info-bar-icon icon-level-beginner\" data-course-level-icon=\"\"></span><span _ngcontent-sc227=\"\" class=\"capitalize\" data-course-level-label=\"\">初级</span></span></div></div><div _ngcontent-sc227=\"\"><!-- --><span _ngcontent-sc227=\"\" class=\"ng-star-inserted\">驱动 AI 产品商业化落地，用 AI 为业务赋能，成为炙手可热稀缺人才。</span></div><!-- --><p _ngcontent-sc227=\"\" class=\"ng-star-inserted\"><span _ngcontent-sc227=\"\" class=\"h4\">合作企业</span> <!-- --><strong _ngcontent-sc227=\"\" class=\"ng-star-inserted\">Figure Eight </strong></p></div></div>\n",
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "summaries = soup.find_all('div', class_='course-summary-card')\n",
    "\n",
    "print(summaries[0])\n",
    "print(type(summaries[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div _ngcontent-sc227=\"\" class=\"course-summary-card row row-gap-medium ng-star-inserted\"><div _ngcontent-sc227=\"\" class=\"col-sm-3\"><!-- --><img _ngcontent-sc227=\"\" alt=\"数据挖掘 求职直通班\" class=\"course-thumb img-responsive img-bordered center-block ng-star-inserted\" height=\"170\" src=\"https://s3.cn-north-1.amazonaws.com.cn/static-assets/course-catalog/nd002-cn-advanced-vip_thumbnail.jpg\" width=\"290\"/><!-- --><!-- --></div><div _ngcontent-sc227=\"\" class=\"col-sm-9 course-text\"><div _ngcontent-sc227=\"\" class=\"row\"><div _ngcontent-sc227=\"\" class=\"col-sm-8\"><h3 _ngcontent-sc227=\"\" class=\"h-slim\"><a _ngcontent-sc227=\"\" href=\"/course/data-mining-engineering-nanodegree--nd002-cn-advanced-vip\"> 数据挖掘 求职直通班</a><span _ngcontent-sc227=\"\" class=\"badges\"><!-- --></span></h3></div><div _ngcontent-sc227=\"\" class=\"col-sm-4 hidden-xs\"><!-- --><span _ngcontent-sc227=\"\" class=\"caption text-right ng-star-inserted\"><span _ngcontent-sc227=\"\" class=\"icon-level course-info-bar-icon icon-level-intermediate\" classname=\"icon-level course-info-bar-icon icon-level-intermediate\" data-course-level-icon=\"\"></span><span _ngcontent-sc227=\"\" class=\"capitalize\" data-course-level-label=\"\">中级</span></span></div></div><div _ngcontent-sc227=\"\"><!-- --><span _ngcontent-sc227=\"\" class=\"ng-star-inserted\">从数据分析、机器学习开始，掌握推荐系统和大数据计算框架 Spark 等核心技能，通过学习、项目、拓展、案例和求职辅导，系统完善对接职场需求，一站式直达职业目标</span></div><!-- --><p _ngcontent-sc227=\"\" class=\"ng-star-inserted\"><span _ngcontent-sc227=\"\" class=\"h4\">合作企业</span> <!-- --><strong _ngcontent-sc227=\"\" class=\"ng-star-inserted\">Tableau </strong><strong _ngcontent-sc227=\"\" class=\"ng-star-inserted\">kaggle </strong><strong _ngcontent-sc227=\"\" class=\"ng-star-inserted\">Starbucks </strong><strong _ngcontent-sc227=\"\" class=\"ng-star-inserted\">IBM Watson </strong><strong _ngcontent-sc227=\"\" class=\"ng-star-inserted\">Bertelsmann </strong></p></div></div>\n",
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "print(summaries[1])\n",
    "print(type(summaries[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI 产品经理 (英)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0].select_one('h3 a').get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(summaries[0].select_one('h3 a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span _ngcontent-sc227=\"\" class=\"caption text-right ng-star-inserted\"><span _ngcontent-sc227=\"\" class=\"icon-level course-info-bar-icon icon-level-beginner\" classname=\"icon-level course-info-bar-icon icon-level-beginner\" data-course-level-icon=\"\"></span><span _ngcontent-sc227=\"\" class=\"capitalize\" data-course-level-label=\"\">初级</span></span>,\n",
       " <span _ngcontent-sc227=\"\" class=\"ng-star-inserted\">驱动 AI 产品商业化落地，用 AI 为业务赋能，成为炙手可热稀缺人才。</span>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _ngcontent-iridium-cn-c20\n",
    "summaries[0].find_all('span','ng-star-inserted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'驱动 AI 产品商业化落地，用 AI 为业务赋能，成为炙手可热稀缺人才。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0].find_all('span', 'ng-star-inserted')[1].get_text().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取标题和简介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "186\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "# 总成标签块\n",
    "tags_summaries = soup.find_all('div', class_='course-summary-card')\n",
    "\n",
    "# 标题\n",
    "lst_titles = []\n",
    "# 等级\n",
    "lst_levels = []\n",
    "# 简介\n",
    "lst_introductions = []\n",
    "\n",
    "for tag in tags_summaries:\n",
    "    lst_titles.append(tag.select_one('h3 a').get_text().strip())\n",
    "    r = tag.find_all('span', 'ng-star-inserted')\n",
    "    try:\n",
    "        lst_introductions.append(r[1].get_text().strip())\n",
    "        lst_levels.append(r[0].get_text().strip())\n",
    "    except IndexError as e:\n",
    "#         print(tag.find_all('span', 'ng-star-inserted')[0].get_text().strip())\n",
    "        lst_introductions.append('None')\n",
    "        lst_levels.append(r[0].get_text().strip())\n",
    "        continue\n",
    "    \n",
    "print(len(lst_titles))\n",
    "print(len(lst_levels))\n",
    "print(len(lst_introductions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('物理入门', '初级', '探访意大利，荷兰和英国物理学中的著名发现的发生地点，学习物理学基础知识。')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_titles[100], lst_levels[100], lst_introductions[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('后端入门', '中级', 'None')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_titles.index('后端入门')\n",
    "\n",
    "lst_titles[92], lst_levels[92], lst_introductions[92]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.\n",
    "“Apart from common word processor operations that treat text like a mere sequence of symbols, NLP considers the hierarchical structure of language: several words make a phrase, several phrases make a sentence and, ultimately, sentences convey ideas,” John Rehling, an NLP expert at Meltwater Group, said in How Natural Language Processing Helps Uncover Social Media Sentiment. “By analyzing language for its meaning, NLP systems have long filled useful roles, such as correcting grammar, converting speech to text and automatically translating between languages.”\n",
    "NLP is used to analyze text, allowing machines to understand how human’s speak. This human-computer interaction enables real-world applications like automatic text summarization, sentiment analysis, topic extraction, named entity recognition, parts-of-speech tagging, relationship extraction, stemming, and more. NLP is commonly used for text mining, machine translation, and automated question answering.\n",
    "NLP is characterized as a hard problem in computer science. Human lan\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sub in module re:\n",
      "\n",
      "sub(pattern, repl, string, count=0, flags=0)\n",
      "    Return the string obtained by replacing the leftmost\n",
      "    non-overlapping occurrences of the pattern in string by the\n",
      "    replacement repl.  repl can be either a string or a callable;\n",
      "    if a string, backslash escapes in it are processed.  If it is\n",
      "    a callable, it's passed the Match object and must return\n",
      "    a replacement string to be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re.sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLP is a way for computers to analyze  understand  and derive meaning from human language in a smart and useful way  By utilizing NLP  developers can organize and structure knowledge to perform tasks such as automatic summarization  translation  named entity recognition  relationship extraction  sentiment analysis  speech recognition  and topic segmentation   Apart from common word processor operations that treat text like a mere sequence of symbols  NLP considers the hierarchical structure of language  several words make a phrase  several phrases make a sentence and  ultimately  sentences convey ideas   John Rehling  an NLP expert at Meltwater Group  said in How Natural Language Processing Helps Uncover Social Media Sentiment   By analyzing language for its meaning  NLP systems have long filled useful roles  such as correcting grammar  converting speech to text and automatically translating between languages   NLP is used to analyze text  allowing machines to understand how human s speak  This human computer interaction enables real world applications like automatic text summarization  sentiment analysis  topic extraction  named entity recognition  parts of speech tagging  relationship extraction  stemming  and more  NLP is commonly used for text mining  machine translation  and automated question answering  NLP is characterized as a hard problem in computer science  Human lan'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "r = re.compile('[a-zA-Z0-9]*')\n",
    "s = re.sub('[^a-zA-Z0-9]', ' ', s)\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP',\n",
       " 'is',\n",
       " 'a',\n",
       " 'way',\n",
       " 'for',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'understand',\n",
       " 'and']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(s)\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP',\n",
       " 'way',\n",
       " 'computers',\n",
       " 'analyze',\n",
       " 'understand',\n",
       " 'derive',\n",
       " 'meaning',\n",
       " 'human',\n",
       " 'language',\n",
       " 'smart']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_cleaned = [w for w in words if w not in stopwords.words('english')]\n",
    "\n",
    "words_cleaned[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLP', 'NNP'),\n",
       " ('way', 'NN'),\n",
       " ('computers', 'NNS'),\n",
       " ('analyze', 'VBP'),\n",
       " ('understand', 'JJ'),\n",
       " ('derive', 'JJ'),\n",
       " ('meaning', 'NN'),\n",
       " ('human', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('smart', 'JJ')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_tags = pos_tag(words_cleaned)\n",
    "\n",
    "words_tags[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命名实体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLP', 'NNP'),\n",
       " ('way', 'NN'),\n",
       " ('computers', 'NNS'),\n",
       " ('analyze', 'VBP'),\n",
       " ('understand', 'JJ'),\n",
       " ('derive', 'JJ'),\n",
       " ('meaning', 'NN'),\n",
       " ('human', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('smart', 'JJ')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_chunk = ne_chunk(words_tags)\n",
    "\n",
    "words_chunk[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 145)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_tags), len(words_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.tree.Tree"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words_chunk[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 \t (ORGANIZATION NLP/NNP)\n",
      "35 \t (PERSON Apart/NNP)\n",
      "46 \t (ORGANIZATION NLP/NNP)\n",
      "63 \t (PERSON John/NNP Rehling/NNP)\n",
      "66 \t (PERSON Meltwater/NNP Group/NNP)\n",
      "68 \t (PERSON How/NNP Natural/NNP Language/NNP)\n",
      "79 \t (ORGANIZATION NLP/NNP)\n",
      "93 \t (ORGANIZATION NLP/NNP)\n",
      "127 \t (ORGANIZATION NLP/NNP)\n",
      "137 \t (ORGANIZATION NLP/NNP)\n",
      "143 \t (PERSON Human/NNP)\n"
     ]
    }
   ],
   "source": [
    "for i in range(145):\n",
    "    if type(words_chunk[i])is not tuple:\n",
    "        print(i, '\\t', words_chunk[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词干提取、词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词干提取\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp',\n",
       " 'way',\n",
       " 'comput',\n",
       " 'analyz',\n",
       " 'understand',\n",
       " 'deriv',\n",
       " 'mean',\n",
       " 'human',\n",
       " 'languag',\n",
       " 'smart']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_porter = [PorterStemmer().stem(w) for w in words_cleaned]\n",
    "\n",
    "words_porter[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词形还原\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP',\n",
       " 'way',\n",
       " 'computers',\n",
       " 'analyze',\n",
       " 'understand',\n",
       " 'derive',\n",
       " 'mean',\n",
       " 'human',\n",
       " 'language',\n",
       " 'smart']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_net = [WordNetLemmatizer().lemmatize(w, pos='v') for w in words_cleaned]\n",
    "\n",
    "words_net[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP NNP\n",
      "way NN\n",
      "computers NNS\n",
      "analyze VBP\n",
      "understand JJ\n",
      "derive JJ\n",
      "meaning NN\n",
      "human JJ\n",
      "language NN\n",
      "smart JJ\n"
     ]
    }
   ],
   "source": [
    "for w,t in words_tags[:10]:\n",
    "    print(w, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method lemmatize in module nltk.stem.wordnet:\n",
      "\n",
      "lemmatize(word, pos='n') method of nltk.stem.wordnet.WordNetLemmatizer instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(WordNetLemmatizer().lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['understand', 'derive', 'meaning', 'human', 'language', 'smart']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = ['understand','derive' ,'meaning' ,'human' ,'language' ,'smart']\n",
    "[WordNetLemmatizer().lemmatize(w, pos='n') for w in ws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ones', 'NNS'), ('was', 'VBD'), ('boring', 'VBG')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['one', 'be', 'bore']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据词性进行词形还原\n",
    "w_t = pos_tag(['ones', 'was', 'boring'])\n",
    "print(w_t)\n",
    "\n",
    "[WordNetLemmatizer().lemmatize(w, pos=t[0].lower()) for w,t in w_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk.tokenize in nltk:\n",
      "\n",
      "NAME\n",
      "    nltk.tokenize - NLTK Tokenizer Package\n",
      "\n",
      "DESCRIPTION\n",
      "    Tokenizers divide strings into lists of substrings.  For example,\n",
      "    tokenizers can be used to find the words and punctuation in a string:\n",
      "    \n",
      "        >>> from nltk.tokenize import word_tokenize\n",
      "        >>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n",
      "        ... two of them.\\n\\nThanks.'''\n",
      "        >>> word_tokenize(s)\n",
      "        ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.',\n",
      "        'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "    \n",
      "    This particular tokenizer requires the Punkt sentence tokenization\n",
      "    models to be installed. NLTK also provides a simpler,\n",
      "    regular-expression based tokenizer, which splits text on whitespace\n",
      "    and punctuation:\n",
      "    \n",
      "        >>> from nltk.tokenize import wordpunct_tokenize\n",
      "        >>> wordpunct_tokenize(s)\n",
      "        ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.',\n",
      "        'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "    \n",
      "    We can also operate at the level of sentences, using the sentence\n",
      "    tokenizer directly as follows:\n",
      "    \n",
      "        >>> from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "        >>> sent_tokenize(s)\n",
      "        ['Good muffins cost $3.88\\nin New York.', 'Please buy me\\ntwo of them.', 'Thanks.']\n",
      "        >>> [word_tokenize(t) for t in sent_tokenize(s)]\n",
      "        [['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.'],\n",
      "        ['Please', 'buy', 'me', 'two', 'of', 'them', '.'], ['Thanks', '.']]\n",
      "    \n",
      "    Caution: when tokenizing a Unicode string, make sure you are not\n",
      "    using an encoded version of the string (it may be necessary to\n",
      "    decode it first, e.g. with ``s.decode(\"utf8\")``.\n",
      "    \n",
      "    NLTK tokenizers can produce token-spans, represented as tuples of integers\n",
      "    having the same semantics as string slices, to support efficient comparison\n",
      "    of tokenizers.  (These methods are implemented as generators.)\n",
      "    \n",
      "        >>> from nltk.tokenize import WhitespaceTokenizer\n",
      "        >>> list(WhitespaceTokenizer().span_tokenize(s))\n",
      "        [(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36), (38, 44),\n",
      "        (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]\n",
      "    \n",
      "    There are numerous ways to tokenize text.  If you need more control over\n",
      "    tokenization, see the other methods provided in this package.\n",
      "    \n",
      "    For further information, please see Chapter 3 of the NLTK book.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    api\n",
      "    casual\n",
      "    mwe\n",
      "    nist\n",
      "    punkt\n",
      "    regexp\n",
      "    repp\n",
      "    sexpr\n",
      "    simple\n",
      "    stanford\n",
      "    stanford_segmenter\n",
      "    texttiling\n",
      "    toktok\n",
      "    treebank\n",
      "    util\n",
      "\n",
      "FUNCTIONS\n",
      "    sent_tokenize(text, language='english')\n",
      "        Return a sentence-tokenized copy of *text*,\n",
      "        using NLTK's recommended sentence tokenizer\n",
      "        (currently :class:`.PunktSentenceTokenizer`\n",
      "        for the specified language).\n",
      "        \n",
      "        :param text: text to split into sentences\n",
      "        :param language: the model name in the Punkt corpus\n",
      "    \n",
      "    word_tokenize(text, language='english', preserve_line=False)\n",
      "        Return a tokenized copy of *text*,\n",
      "        using NLTK's recommended word tokenizer\n",
      "        (currently an improved :class:`.TreebankWordTokenizer`\n",
      "        along with :class:`.PunktSentenceTokenizer`\n",
      "        for the specified language).\n",
      "        \n",
      "        :param text: text to split into words\n",
      "        :type text: str\n",
      "        :param language: the model name in the Punkt corpus\n",
      "        :type language: str\n",
      "        :param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\n",
      "        :type preserve_line: bool\n",
      "\n",
      "DATA\n",
      "    improved_close_quote_regex = re.compile('([»”’])')\n",
      "    improved_open_quote_regex = re.compile('([«“‘„]|[`]+)')\n",
      "    improved_open_single_quote_regex = re.compile(\"(?i)(\\\\')(?!re|ve|ll|m|...\n",
      "    improved_punct_regex = re.compile('([^\\\\.])(\\\\.)([\\\\]\\\\)}>\"\\\\\\'»”’ ]*)...\n",
      "\n",
      "FILE\n",
      "    d:\\setup_space_all\\anaconda\\lib\\site-packages\\nltk\\tokenize\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __END__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
