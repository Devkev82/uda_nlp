{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'As I was waiting, a man came out of a side room, and at a glance I was sure he must be Long John. \\\n",
    "His left leg was cut off close by the hip, and under the left shoulder he carried a crutch, which he managed \\\n",
    "with wonderful dexterity, hopping about upon it like a bird. He was very tall and strong, with a face as big \\\n",
    "as a ham—plain and pale, but intelligent and smiling. Indeed, he seemed in the most cheerful spirits, whistling \\\n",
    "as he moved about among the tables, with a merry word or a slap on the shoulder for the more favoured of his guests.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_low = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as i was waiting, a man came out of a side room, and at a glance i was sure he must be long john. his left leg was cut off close by the hip, and under the left shoulder he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird. he was very tall and strong, with a face as big as a ham—plain and pale, but intelligent and smiling. indeed, he seemed in the most cheerful spirits, whistling as he moved about among the tables, with a merry word or a slap on the shoulder for the more favoured of his guests.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "s = re.compile('[a-zA-Z]*')\n",
    "l = [i for i in s.findall(text_low) if len(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3,3,2,4]\n",
    "d = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function get:\n",
      "\n",
      "get(key, default=None, /) method of builtins.dict instance\n",
      "    Return the value for key if key is in the dictionary, else default.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(d.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in l:\n",
    "    d[n] = d.get(n, 0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 2, 3: 2, 4: 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getUrlSpecifyText(url='https://cn.udacity.com/courses/all'):\n",
    "    l = []\n",
    "    \n",
    "    req = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    \n",
    "    summaries = soup.find_all('div', class_='course-summary-card')\n",
    "    for tag in summaries:\n",
    "        l_sub = []\n",
    "        \n",
    "        l_sub.append(tag.select_one('h3 a').get_text().strip())          # 课程名称\n",
    "        \n",
    "        r = tag.find_all('span', 'ng-star-inserted')                     # 课程等级\n",
    "        l_sub.append(r[0].get_text().strip())\n",
    "        try:\n",
    "            l_sub.append(r[1].get_text().strip())                        # 课程简介\n",
    "        except IndexError as e:\n",
    "            l_sub.append('None')                                         # 简介为空，则设为‘None’\n",
    "            continue\n",
    "             \n",
    "        l.append(l_sub)\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AI 产品经理 (英)', '初级', '驱动 AI 产品商业化落地，用 AI 为业务赋能，成为炙手可热稀缺人才。'],\n",
       " ['数据挖掘 求职直通班',\n",
       "  '中级',\n",
       "  '从数据分析、机器学习开始，掌握推荐系统和大数据计算框架 Spark 等核心技能，通过学习、项目、拓展、案例和求职辅导，系统完善对接职场需求，一站式直达职业目标'],\n",
       " ['市场营销分析 (英)',\n",
       "  '初级',\n",
       "  '掌握市场营销所需的基本数据技能，使用 Google Analytics 和 Data Studio 呈现你的结论'],\n",
       " ['传感器融合 (英)', '高级', '和梅赛德斯-奔驰学习，成为掌握无人驾驶、物联网、机器人开发核心技术的抢手工程师。'],\n",
       " ['数据洞察与说服技巧 (英)', '中级', '掌握数据可视化呈现商业洞察，练就用数据影响决策的说服技巧。'],\n",
       " ['云计算软件开发 (英)',\n",
       "  '中级',\n",
       "  '学习在 AWS 上构建全栈应用，并使用 Kubernetes 和 Serverless 框架分发采用微服务的应用，成为炙手可热的云计算软件开发工程师。'],\n",
       " ['云计算 DevOps (英)',\n",
       "  '中级',\n",
       "  '学习在 AWS 上通过代码部署基础设施和应用，构建 CI/CD 管道，以及使用 Kubernetes 和其他现代工具运维化微服务，成为一名抢手的云计算 DevOps 工程师。'],\n",
       " ['AI 求职直通班',\n",
       "  '中级',\n",
       "  '一站掌握机器学习、深度学习、CV、NLP 等 AI 核心技能，覆盖金融风控、计算机视觉、自然语言处理、数据挖掘、计算广告、语音识别六大热门领域，高性价比对接职场需求'],\n",
       " ['C++ 程序设计 (英)', '中级', '全面提升面向对象编程能力，成为 C++ 程序设计的专家。'],\n",
       " ['数据结构与算法 (英)', '中级', '精选 100+ 道面试真题，挑战数据结构与算法习题，获得详细项目审阅反馈，轻松应对求职面试和工作挑战。']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getUrlSpecifyText()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. \n",
    "By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, \n",
    "named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.\n",
    "“Apart from common word processor operations that treat text like a mere sequence of symbols, NLP considers the hierarchical \n",
    "structure of language: several words make a phrase, several phrases make a sentence and, ultimately, sentences convey ideas,” \n",
    "John Rehling, an NLP expert at Meltwater Group, said in How Natural Language Processing Helps Uncover Social Media Sentiment. \n",
    "“By analyzing language for its meaning, NLP systems have long filled useful roles, such as correcting grammar, converting speech \n",
    "to text and automatically translating between languages.”\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def getNormalizedText(text=s):\n",
    "    s_c = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    \n",
    "    # tokenize\n",
    "    words = word_tokenize(s_c)\n",
    "    \n",
    "    # stopwords\n",
    "    w_s = [w for w in words if w not in stopwords.words('english')]\n",
    "    \n",
    "    return w_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP',\n",
       " 'way',\n",
       " 'computers',\n",
       " 'analyze',\n",
       " 'understand',\n",
       " 'derive',\n",
       " 'meaning',\n",
       " 'human',\n",
       " 'language',\n",
       " 'smart']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getNormalizedText()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    # 初始化神经网络（各层节点数、学习率） --各层节点数、学习率、2个权重矩阵、激活函数\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # 输入层、隐藏层、输出层的节点数量\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # 学习率\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        ## 权重矩阵（一列）  --numpy.random.normal\n",
    "        # 权重数组中的w_i_j，表示链接是从下一层的节点i到节点j --使用正态分布采样权重\n",
    "        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        # 设置激活函数S -scipy.special.expit(x)\n",
    "        self.activation_function = lambda x: 1/(1+numpy.exp(-x))\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # 训练神经网络（训练数据集整理后的输入列表，目标数值列表） --train数据集输入，训练神经网络，调整权重\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # 将输入列表转换为二维数组并转置\n",
    "        # 将目标列表转换为二维数组并转置\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # 矩阵点乘法--计算进入隐藏层的信号\n",
    "        # 激活函数--计算离开隐藏层的信号\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # 矩阵点乘法--计算进入最终输出层的信号\n",
    "        # 激活函数--计算离开最终输出层的信号（最终输出）\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        print('final_outputs', final_outputs)\n",
    "        \n",
    "        # 隐蔽层和最终层的误差\n",
    "        # 输入层和隐藏层的误差\n",
    "        print('targets', targets)\n",
    "        output_errors = targets - final_outputs\n",
    "        print('output_errors', output_errors)\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        print('hidden_errors', hidden_errors)\n",
    "        \n",
    "        # 为隐藏层和最终层间权重矩阵进行更新\n",
    "        # 为输入层和隐藏层间权重矩阵进行更新\n",
    "        print('who_term', (output_errors * final_outputs * (1.0 - final_outputs)))\n",
    "        print('wih_term', (hidden_errors * hidden_outputs * (1.0 - hidden_outputs)))\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        \n",
    "    # 查询神经网络（测试数据集整理后的输入列表） --训练后，test数据集输入，返回计算得出的检验样本集最终输出\n",
    "    def query(self, inputs_list):\n",
    "        # 将输入列表转换为二维数组并转置\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # 矩阵点乘法--计算进入隐藏层的信号\n",
    "        # 激活函数--计算离开隐藏层的信号\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # 矩阵点乘法--计算进入最终输出层的信号\n",
    "        # 激活函数--计算离开最终输出层的信号（最终输出）\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # 返回最终输出列表\n",
    "        return final_outputs\n",
    "    \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4).reshape((2,2))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [1, 3]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.transpose(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [1, 3]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[0.5, -0.2, 0.1]])\n",
    "target = np.array([[0.4]])\n",
    "learnrate = 0.5\n",
    "\n",
    "# 3*2\n",
    "weights_input_hidden = np.array([[0.1, -0.2],\n",
    "                                 [0.4, 0.5],\n",
    "                                 [-0.3, 0.2]]).T\n",
    "# 2*1\n",
    "weights_hidden_output = np.array([[0.3],\n",
    "                                  [-0.1]]).T\n",
    "\n",
    "delta_weights_i_h = np.zeros(weights_input_hidden.shape)\n",
    "delta_weights_h_o = np.zeros(weights_hidden_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.1,  0.4, -0.3],\n",
       "        [-0.2,  0.5,  0.2]]), array([[ 0.3, -0.1]]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_input_hidden, weights_hidden_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例化（测试train）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = neuralNetwork(3,2,1,0.1)\n",
    "\n",
    "nn.wih = weights_input_hidden.copy()\n",
    "nn.who = weights_hidden_output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5, -0.2, 0.1], [0.4])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转为list of list\n",
    "X = list(x[0])\n",
    "y = list(target[0])\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_outputs [[0.5249765]]\n",
      "targets [[0.4]]\n",
      "output_errors [[-0.1249765]]\n",
      "hidden_errors [[-0.03749295]\n",
      " [ 0.01249765]]\n",
      "who_term [[-0.03116616]]\n",
      "wih_term [[-0.00936481]\n",
      " [ 0.00309924]]\n"
     ]
    }
   ],
   "source": [
    "nn.train(x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29848843, -0.10141844]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09953176,  0.4001873 , -0.30009365],\n",
       "       [-0.19984504,  0.49993802,  0.20003099]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.wih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(nn.who, np.array([[ 0.37275328], [-0.03172939]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(nn.wih,np.array([[ 0.10562014, -0.20185996], \n",
    "                              [0.39775194, 0.50074398], \n",
    "                              [-0.29887597, 0.19962801]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = lambda x:max(0, x)\n",
    "y(-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\\n",
    "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow basic process (for mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data                                        # mnist data set\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\"\"\"mini-batch\"\"\"\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    output_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0    - Cost: 1.7      Valid Accuracy: 0.741\n",
      "Epoch: 10   - Cost: 0.629    Valid Accuracy: 0.878\n",
      "Epoch: 20   - Cost: 0.509    Valid Accuracy: 0.893\n",
      "Epoch: 30   - Cost: 0.444    Valid Accuracy: 0.899\n",
      "Epoch: 40   - Cost: 0.402    Valid Accuracy: 0.902\n",
      "Epoch: 50   - Cost: 0.372    Valid Accuracy: 0.905\n",
      "Epoch: 60   - Cost: 0.349    Valid Accuracy: 0.908\n",
      "Epoch: 70   - Cost: 0.331    Valid Accuracy: 0.91 \n",
      "Epoch: 80   - Cost: 0.315    Valid Accuracy: 0.913\n",
      "Epoch: 90   - Cost: 0.302    Valid Accuracy: 0.914\n",
      "Test Accuracy: 0.9143000245094299\n"
     ]
    }
   ],
   "source": [
    "\"\"\"hyper-parameters define\"\"\"\n",
    "batch_size = 128                                            # mini-batch size\n",
    "epochs = 100                                                # epoch size\n",
    "learn_rate = 0.1\n",
    "\n",
    "\n",
    "\"\"\"data importation\"\"\"\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# mini-batch operation\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "\"\"\"model parameters define\"\"\"\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "\"\"\"TensorFlow frame parameters define\"\"\"\n",
    "# Features and Labels pre-define\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Logits is composed of weights and bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))           # weights\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))                       # bias\n",
    "logits = tf.add(tf.matmul(features, weights), bias)                     # logis\n",
    "\n",
    "# Learning  rate pre-define\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# Define loss by logits and labels\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "# Define optimizer by learning rate and loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy by 'equal', 'cast' and 'reduce_mean' functions\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()                    # globle variables pre-define\n",
    "\n",
    "\"\"\"TensorFlow Session(main processing)\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)                                         # globle variables define\n",
    "\n",
    "    # Training ALL data 'epoch' times\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Training data BATCH-by-BATCH a time\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            # re-define 'feed_dict' each time(used to transfer every batch of data included features and labels)\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            # run optimizer by 'feed_dict' to optimize model\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "            \n",
    "        # Calculate loss by every batch of labels\n",
    "        current_cost = sess.run(cost, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "        # Calculate accuracy by validation data\n",
    "        valid_accuracy = sess.run(accuracy, feed_dict={features: valid_features, labels: valid_labels})\n",
    "\n",
    "        # epoch_i, sess, batch_features, batch_labels\n",
    "        \n",
    "        if epoch_i%10==0:\n",
    "            print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "            epoch_i,\n",
    "            current_cost,\n",
    "            valid_accuracy))\n",
    "        \n",
    "    # Calculate accuracy for test dataset(target accuracy)\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
