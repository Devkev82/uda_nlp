{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'As I was waiting, a man came out of a side room, and at a glance I was sure he must be Long John. \\\n",
    "His left leg was cut off close by the hip, and under the left shoulder he carried a crutch, which he managed \\\n",
    "with wonderful dexterity, hopping about upon it like a bird. He was very tall and strong, with a face as big \\\n",
    "as a ham—plain and pale, but intelligent and smiling. Indeed, he seemed in the most cheerful spirits, whistling \\\n",
    "as he moved about among the tables, with a merry word or a slap on the shoulder for the more favoured of his guests.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_low = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as i was waiting, a man came out of a side room, and at a glance i was sure he must be long john. his left leg was cut off close by the hip, and under the left shoulder he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird. he was very tall and strong, with a face as big as a ham—plain and pale, but intelligent and smiling. indeed, he seemed in the most cheerful spirits, whistling as he moved about among the tables, with a merry word or a slap on the shoulder for the more favoured of his guests.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "s = re.compile('[a-zA-Z]*')\n",
    "l = [i for i in s.findall(text_low) if len(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3,3,2,4]\n",
    "d = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function get:\n",
      "\n",
      "get(key, default=None, /) method of builtins.dict instance\n",
      "    Return the value for key if key is in the dictionary, else default.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(d.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in l:\n",
    "    d[n] = d.get(n, 0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 2, 3: 2, 4: 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getUrlSpecifyText(url='https://cn.udacity.com/courses/all'):\n",
    "    l = []\n",
    "    \n",
    "    req = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    \n",
    "    summaries = soup.find_all('div', class_='course-summary-card')\n",
    "    for tag in summaries:\n",
    "        l_sub = []\n",
    "        \n",
    "        l_sub.append(tag.select_one('h3 a').get_text().strip())          # 课程名称\n",
    "        \n",
    "        r = tag.find_all('span', 'ng-star-inserted')                     # 课程等级\n",
    "        l_sub.append(r[0].get_text().strip())\n",
    "        try:\n",
    "            l_sub.append(r[1].get_text().strip())                        # 课程简介\n",
    "        except IndexError as e:\n",
    "            l_sub.append('None')                                         # 简介为空，则设为‘None’\n",
    "            continue\n",
    "             \n",
    "        l.append(l_sub)\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AI 产品经理 (英)', '初级', '驱动 AI 产品商业化落地，用 AI 为业务赋能，成为炙手可热稀缺人才。'],\n",
       " ['数据挖掘 求职直通班',\n",
       "  '中级',\n",
       "  '从数据分析、机器学习开始，掌握推荐系统和大数据计算框架 Spark 等核心技能，通过学习、项目、拓展、案例和求职辅导，系统完善对接职场需求，一站式直达职业目标'],\n",
       " ['市场营销分析 (英)',\n",
       "  '初级',\n",
       "  '掌握市场营销所需的基本数据技能，使用 Google Analytics 和 Data Studio 呈现你的结论'],\n",
       " ['传感器融合 (英)', '高级', '和梅赛德斯-奔驰学习，成为掌握无人驾驶、物联网、机器人开发核心技术的抢手工程师。'],\n",
       " ['数据洞察与说服技巧 (英)', '中级', '掌握数据可视化呈现商业洞察，练就用数据影响决策的说服技巧。'],\n",
       " ['云计算软件开发 (英)',\n",
       "  '中级',\n",
       "  '学习在 AWS 上构建全栈应用，并使用 Kubernetes 和 Serverless 框架分发采用微服务的应用，成为炙手可热的云计算软件开发工程师。'],\n",
       " ['云计算 DevOps (英)',\n",
       "  '中级',\n",
       "  '学习在 AWS 上通过代码部署基础设施和应用，构建 CI/CD 管道，以及使用 Kubernetes 和其他现代工具运维化微服务，成为一名抢手的云计算 DevOps 工程师。'],\n",
       " ['AI 求职直通班',\n",
       "  '中级',\n",
       "  '一站掌握机器学习、深度学习、CV、NLP 等 AI 核心技能，覆盖金融风控、计算机视觉、自然语言处理、数据挖掘、计算广告、语音识别六大热门领域，高性价比对接职场需求'],\n",
       " ['C++ 程序设计 (英)', '中级', '全面提升面向对象编程能力，成为 C++ 程序设计的专家。'],\n",
       " ['数据结构与算法 (英)', '中级', '精选 100+ 道面试真题，挑战数据结构与算法习题，获得详细项目审阅反馈，轻松应对求职面试和工作挑战。']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getUrlSpecifyText()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. \n",
    "By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, \n",
    "named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.\n",
    "“Apart from common word processor operations that treat text like a mere sequence of symbols, NLP considers the hierarchical \n",
    "structure of language: several words make a phrase, several phrases make a sentence and, ultimately, sentences convey ideas,” \n",
    "John Rehling, an NLP expert at Meltwater Group, said in How Natural Language Processing Helps Uncover Social Media Sentiment. \n",
    "“By analyzing language for its meaning, NLP systems have long filled useful roles, such as correcting grammar, converting speech \n",
    "to text and automatically translating between languages.”\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def getNormalizedText(text=s):\n",
    "    s_c = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    \n",
    "    # tokenize\n",
    "    words = word_tokenize(s_c)\n",
    "    \n",
    "    # stopwords\n",
    "    w_s = [w for w in words if w not in stopwords.words('english')]\n",
    "    \n",
    "    return w_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP',\n",
       " 'way',\n",
       " 'computers',\n",
       " 'analyze',\n",
       " 'understand',\n",
       " 'derive',\n",
       " 'meaning',\n",
       " 'human',\n",
       " 'language',\n",
       " 'smart']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getNormalizedText()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句子解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#  nltk.ChartParser(my_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define a custom grammar\n",
    "my_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(my_grammar)\n",
    "\n",
    "# Parse a sentence\n",
    "sentence = word_tokenize(\"I shot an elephant in my pajamas\")\n",
    "for tree in parser.parse(sentence):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e613bbfcd985>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Reduce words to their stems\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mstemmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemmed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-973660e6e5fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Reduce words to their root form\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlemmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize verbs by specifying pos\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, \n",
    "                                       (self.input_nodes, self.hidden_nodes))\n",
    "\n",
    "        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
    "                                       (self.hidden_nodes, self.output_nodes))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        #### TODO: Set self.activation_function to your implemented sigmoid function ####\n",
    "        #\n",
    "        # Note: in Python, you can define a function with a lambda expression,\n",
    "        # as shown below.\n",
    "        self.activation_function = lambda x :  1 / (1+np.exp(-x))# Replace 0 with your sigmoid calculation.\n",
    "        \n",
    "        ### If the lambda code above is not something you're familiar with,\n",
    "        # You can uncomment out the following three lines and put your \n",
    "        # implementation there instead.\n",
    "        #\n",
    "        #def sigmoid(x):\n",
    "        #    return 0  # Replace 0 with your sigmoid calculation here\n",
    "        #self.activation_function = sigmoid\n",
    "                    \n",
    "    \n",
    "    def train(self, features, targets):\n",
    "        ''' Train the network on batch of features and targets. \n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            \n",
    "            features: 2D array, each row is one data record, each column is a feature\n",
    "            targets: 1D array of target values\n",
    "        \n",
    "        '''\n",
    "        n_records = features.shape[0]\n",
    "        delta_weights_i_h = np.zeros(self.weights_input_to_hidden.shape)\n",
    "        delta_weights_h_o = np.zeros(self.weights_hidden_to_output.shape)\n",
    "        for X, y in zip(features, targets):\n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "            # TODO: Hidden layer - Replace these values with your calculations.\n",
    "            hidden_inputs = np.dot(X, self.weights_input_to_hidden) # signals into hidden layer\n",
    "            hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
    "\n",
    "            # TODO: Output layer - Replace these values with your calculations.\n",
    "            final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output) # signals into final output layer\n",
    "            final_outputs = self.activation_function(final_inputs) # signals from final output layer\n",
    "            \n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # TODO: Output error - Replace this value with your calculations.\n",
    "            error = y - final_outputs # Output layer error is the difference between desired target and actual output.\n",
    "            # error_term = error * final_outputs * (1-final_outputs)\n",
    "            \n",
    "            # TODO: Calculate the hidden layer's contribution to the error\n",
    "            hidden_error = np.dot(error * final_outputs * (1-final_outputs), final_inputs)\n",
    "            # hidden_error_term = hidden_error * hidden_outputs * (1-hidden_outputs)\n",
    "            \n",
    "            # TODO: Backpropagated error terms - Replace these values with your calculations.\n",
    "            output_error_term = error * final_outputs * (1-final_outputs)\n",
    "            hidden_error_term = hidden_error * hidden_outputs * (1-hidden_outputs)\n",
    "\n",
    "            # Weight step (input to hidden)\n",
    "            delta_weights_i_h += hidden_error_term * X[:, None]\n",
    "            # Weight step (hidden to output)\n",
    "            delta_weights_h_o += (output_error_term * hidden_outputs)[:, None]\n",
    "\n",
    "        # TODO: Update the weights - Replace these values with your calculations.\n",
    "        self.weights_hidden_to_output += self.lr * delta_weights_h_o / n_records # update hidden-to-output weights with gradient descent step\n",
    "        self.weights_input_to_hidden += self.lr * delta_weights_i_h / n_records # update input-to-hidden weights with gradient descent step\n",
    " \n",
    "    def run(self, features):\n",
    "        ''' Run a forward pass through the network with input features \n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            features: 1D array of feature values\n",
    "        '''\n",
    "        \n",
    "        #### Implement the forward pass here ####\n",
    "        # TODO: Hidden layer - replace these values with the appropriate calculations.\n",
    "        hidden_inputs =  np.dot(features, self.weights_input_to_hidden) # signals into hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
    "        \n",
    "        # TODO: Output layer - Replace these values with the appropriate calculations.\n",
    "        final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output) # signals into final output layer\n",
    "        final_outputs = self.activation_function(final_inputs) # signals from final output layer \n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "inputs = np.array([[0.5, -0.2, 0.1]])\n",
    "targets = np.array([[0.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01511573, -0.01418438])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([-0.03116616]) * np.array([0.4850045,0.45512111]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose([1e10,1e-8], [1.00001e10,1e-9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose([1e10,1e-8], [1e10,1e-9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    # 初始化神经网络（各层节点数、学习率） --各层节点数、学习率、2个权重矩阵、激活函数\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # 输入层、隐藏层、输出层的节点数量\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # 学习率\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        ## 权重矩阵（一列）  --numpy.random.normal\n",
    "        # 权重数组中的w_i_j，表示链接是从下一层的节点i到节点j --使用正态分布采样权重\n",
    "        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        # 设置激活函数S -scipy.special.expit(x)\n",
    "        self.activation_function = lambda x: 1/(1+numpy.exp(-x))\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # 训练神经网络（训练数据集整理后的输入列表，目标数值列表） --train数据集输入，训练神经网络，调整权重\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # 将输入列表转换为二维数组并转置\n",
    "        # 将目标列表转换为二维数组并转置\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # 矩阵点乘法--计算进入隐藏层的信号\n",
    "        # 激活函数--计算离开隐藏层的信号\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # 矩阵点乘法--计算进入最终输出层的信号\n",
    "        # 激活函数--计算离开最终输出层的信号（最终输出）\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        print('final_outputs', final_outputs)\n",
    "        \n",
    "        # 隐蔽层和最终层的误差\n",
    "        # 输入层和隐藏层的误差\n",
    "        print('targets', targets)\n",
    "        output_errors = targets - final_outputs\n",
    "        print('output_errors', output_errors)\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        print('hidden_errors', hidden_errors)\n",
    "        \n",
    "        # 为隐藏层和最终层间权重矩阵进行更新\n",
    "        # 为输入层和隐藏层间权重矩阵进行更新\n",
    "        print('who_term', (output_errors * final_outputs * (1.0 - final_outputs)))\n",
    "        print('wih_term', (hidden_errors * hidden_outputs * (1.0 - hidden_outputs)))\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        \n",
    "    # 查询神经网络（测试数据集整理后的输入列表） --训练后，test数据集输入，返回计算得出的检验样本集最终输出\n",
    "    def query(self, inputs_list):\n",
    "        # 将输入列表转换为二维数组并转置\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # 矩阵点乘法--计算进入隐藏层的信号\n",
    "        # 激活函数--计算离开隐藏层的信号\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # 矩阵点乘法--计算进入最终输出层的信号\n",
    "        # 激活函数--计算离开最终输出层的信号（最终输出）\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # 返回最终输出列表\n",
    "        return final_outputs\n",
    "    \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4).reshape((2,2))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [1, 3]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.transpose(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [1, 3]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[0.5, -0.2, 0.1]])\n",
    "target = np.array([[0.4]])\n",
    "learnrate = 0.5\n",
    "\n",
    "# 3*2\n",
    "weights_input_hidden = np.array([[0.1, -0.2],\n",
    "                                 [0.4, 0.5],\n",
    "                                 [-0.3, 0.2]]).T\n",
    "# 2*1\n",
    "weights_hidden_output = np.array([[0.3],\n",
    "                                  [-0.1]]).T\n",
    "\n",
    "delta_weights_i_h = np.zeros(weights_input_hidden.shape)\n",
    "delta_weights_h_o = np.zeros(weights_hidden_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.1,  0.4, -0.3],\n",
       "        [-0.2,  0.5,  0.2]]), array([[ 0.3, -0.1]]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_input_hidden, weights_hidden_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例化（测试train）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = neuralNetwork(3,2,1,0.1)\n",
    "\n",
    "nn.wih = weights_input_hidden.copy()\n",
    "nn.who = weights_hidden_output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5, -0.2, 0.1], [0.4])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转为list of list\n",
    "X = list(x[0])\n",
    "y = list(target[0])\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_outputs [[0.5249765]]\n",
      "targets [[0.4]]\n",
      "output_errors [[-0.1249765]]\n",
      "hidden_errors [[-0.03749295]\n",
      " [ 0.01249765]]\n",
      "who_term [[-0.03116616]]\n",
      "wih_term [[-0.00936481]\n",
      " [ 0.00309924]]\n"
     ]
    }
   ],
   "source": [
    "nn.train(x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29848843, -0.10141844]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09953176,  0.4001873 , -0.30009365],\n",
       "       [-0.19984504,  0.49993802,  0.20003099]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.wih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(nn.who, np.array([[ 0.37275328], [-0.03172939]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(nn.wih,np.array([[ 0.10562014, -0.20185996], \n",
    "                              [0.39775194, 0.50074398], \n",
    "                              [-0.29887597, 0.19962801]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = lambda x:max(0, x)\n",
    "y(-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
