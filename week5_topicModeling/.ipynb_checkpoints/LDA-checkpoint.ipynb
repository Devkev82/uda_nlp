{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 0 步：潜在狄利克雷分布 ##\n",
    "\n",
    "LDA 用于将文档中的文本分类为特定话题。它会用狄利克雷分布构建一个话题/文档模型和单词/话题模型。\n",
    "\n",
    "* 每个文档都建模为话题多态分布，每个话题建模为单词多态分布。\n",
    "* LDA 假设我们传入其中的每段文本都相互关联。因此，选择正确的语料库很关键。\n",
    "* 它还假设文档是根据多种话题创建的。然后，这些话题根据单词的分布概率生成单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 1 步：加载数据集\n",
    "\n",
    "我们将使用的数据集是一个列表，其中包含在 15 年内发表的超过 100 万条新闻标题。首先，我们将从 `abcnews-date-text.csv` 文件中加载该数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load the dataset from the CSV and save it to 'data_text'\n",
    "'''\n",
    "data = pd.read_csv('./abcnews-date-text.csv', error_bad_lines=False);\n",
    "# We only need the Headlines text column from the data\n",
    "data_text = data[:300000][['headline_text']];\n",
    "data_text['index'] = data_text.index\n",
    "\n",
    "documents = data_text\n",
    "\n",
    "print(len(data_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 2 步：预处理数据 ##\n",
    "\n",
    "我们将执行以下步骤：\n",
    "\n",
    "* **标记化**：将文本拆分为句子，将句子拆分为单词。使单词全小写并删除标点。\n",
    "* 删除少于 3 个字符的单词。\n",
    "* 删除所有**停止词**。\n",
    "* **词形还原**单词 - 第三人称的单词变成第一人称，过去式和将来式变成现在式。\n",
    "* **词干提取**单词 - 将单词简化成根形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Gensim and nltk libraries\n",
    "'''\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer               # 词形还原、词干提取\n",
    "from nltk.stem.porter import *                                     \n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "[nltk_data] Downloading package wordnet to\n",
    "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
    "[nltk_data]   Package wordnet is already up-to-date!\n",
    "Out[14]:\n",
    "True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer 示例\n",
    "在预处理数据集之前，我们先看一个词形还原示例。如果词形还原单词“went”，输出是什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos = 'v')) # past tense to present tense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmer 示例\n",
    "再看一个词干提取示例。我们向 stemmer 中传入多个单词，看看它是如何处理每个单词的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "\n",
    "pd.DataFrame(data={'original word':original_words, 'stemmed':singles })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    \"\"\"\n",
    "    动词词性词形还原text\n",
    "    \"\"\"\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    l_token = gensim.utils.simple_preprocess(text)                                  # 标记化文本\n",
    "    for token in  l_token:\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3: # 仅处理非停用词\n",
    "            # TODO: Apply lemmatize_stemming() on the token, then add to the results list\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview a document after preprocessing\n",
    "'''\n",
    "document_num = 4310\n",
    "doc_sample = documents[documents['index'] == document_num+1].values[0][0]\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在预处理所有新闻标题。为此，我们使用 pandas 中的 [map](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html) 函数向 `headline_text` 列应用 `preprocess()`。\n",
    "\n",
    "**注意**：可能需要几分钟（我的笔记本需要 6 分钟）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TODO: preprocess all the headlines, saving the list of results as 'processed_docs'\n",
    "processed_docs = [preprocess(text) for text in documents['headline_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['decid', 'communiti', 'broadcast', 'licenc'],\n",
       " ['wit', 'awar', 'defam'],\n",
       " ['call', 'infrastructur', 'protect', 'summit'],\n",
       " ['staff', 'aust', 'strike', 'rise'],\n",
       " ['strike', 'affect', 'australian', 'travel'],\n",
       " ['ambiti', 'olsson', 'win', 'tripl', 'jump'],\n",
       " ['antic', 'delight', 'record', 'break', 'barca'],\n",
       " ['aussi', 'qualifi', 'stosur', 'wast', 'memphi', 'match'],\n",
       " ['aust', 'address', 'secur', 'council', 'iraq'],\n",
       " ['australia', 'lock', 'timet']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Preview 'processed_docs'\n",
    "'''\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 3.1 步：数据集上的词袋\n",
    "\n",
    "现在，根据 processed_docs 创建一个字典，后者包含单词在训练集中的出现次数。为此，将 `processed_docs` 传入 [`gensim.corpora.Dictionary()`](https://radimrehurek.com/gensim/corpora/dictionary.html) 并称之为 `dictionary`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.corpora.dictionary.Dictionary'> 24903 [0, 1, 2, 3, 4]\n",
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "print(type(dictionary), len(dictionary), dictionary.keys()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter_extremes\n",
    "** Gensim filter_extremes **\n",
    "\n",
    "[`filter_extremes(no_below=5, no_above=0.5, keep_n=100000)`](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes)\n",
    "\n",
    "滤除出现在以下情形中的标记\n",
    "\n",
    "* 出现在 no_below 个以下的文档中（绝对数字），或\n",
    "* 出现在 no_above 个以上的文档中（ 总语料库大小的一部分，不是绝对数字）。\n",
    "* 在 (1) 和 (2) 之后，仅保留前 keep_n 个最常见的标记（如果为 None，则保留所有标记）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "# TODO: apply dictionary.filter_extremes() with the parameters mentioned above\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2bow\n",
    "** Gensim doc2bow **\n",
    "\n",
    "[`doc2bow(document)`](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow)\n",
    "\n",
    "* 将文档（单词列表）转换为词袋格式 = 2 元组（token_id、token_count）列表。每个单词都应该是标记化和标准化的字符串（unicode 或 utf8-编码）。文档中的单词没有进一步预处理了；在调用此函数之前，请应用标记化、词干提取等方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "# TODO\n",
    "bow_corpus = [dictionary.doc2bow(words) for words in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(71, 1), (107, 1), (462, 1), (3530, 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Checking Bag of Words corpus for our sample document --> (token_id, token_count)\n",
    "'''\n",
    "bow_corpus[document_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 71 (\"bushfir\") appears 1 time.\n",
      "Word 107 (\"help\") appears 1 time.\n",
      "Word 462 (\"rain\") appears 1 time.\n",
      "Word 3530 (\"dampen\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "# Here document_num is document number 4310 which we have checked in Step 2\n",
    "bow_doc_4310 = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 3.2 步： 对文档集合应用 TF-IDF ##\n",
    "\n",
    "虽然使用 gensim 模型的 LDA 实现并非必须对语料库执行 TF-IDF，但是建议这么做。TF-IDF 在初始化过程中要求词袋（整数值）训练语料库。在转换过程中，它将接受向量并返回另一个维度相同的向量。\n",
    "\n",
    "*请注意：Gensim 的作者规定 LDA 的标准流程是使用词袋模型。*\n",
    "\n",
    "** TF-IDF 是“词频、逆文本频率\"的简称。**\n",
    "\n",
    "* 它是根据单词在多个文档中的出现频率对单词（或“术语”）重要性进行评分的方式。\n",
    "* 如果单词频繁出现在文档中，则很重要，给该单词评很高的得分。但是如果单词出现在很多文档中，则不是唯一标识符，给该单词评很低的得分。\n",
    "* 因此，“the”和“for”等常见单词出现在很多文档中，评分将降低。经常出现在单个文档中的单词评分将升高。\n",
    "\n",
    "换句话说：\n",
    "\n",
    "* TF(w) = `（术语 w 出现在文档中的次数）/（文档中的术语总数）`。\n",
    "* IDF(w) = `log_e（文档总数/包含术语 w 的文档数）`。\n",
    "\n",
    "** 例如 **\n",
    "\n",
    "* 假设有一个文档包含 `100` 个单词，其中单词“tiger”出现了 3 次。\n",
    "* \"tiger\"的词频（即 tf）是：\n",
    "    - `TF = (3 / 100) = 0.03`. \n",
    "\n",
    "* 现在，假设有 `10 million` 个文档，单词”tiger“出现在了其中 `1000` 个文档中。逆文档频率（即 idk）的计算方式为：\n",
    "    - `IDF = log(10,000,000 / 1,000) = 4`. \n",
    "\n",
    "* 因此，Tf-idf 权重是这些数量的积：\n",
    "    - `TF-IDF = 0.03 * 4 = 0.12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel                         # TFIDF初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create tf-idf model object using models.TfidfModel on 'bow_corpus' and save it to 'tfidf'\n",
    "'''\n",
    "# TODO\n",
    "tfidf = TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Apply transformation to the entire corpus and call it 'corpus_tfidf'\n",
    "'''\n",
    "# TODO\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5959813347777092),\n",
      " (1, 0.39204529549491984),\n",
      " (2, 0.48531419274988147),\n",
      " (3, 0.5055461098578569)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview TF-IDF scores for our first document --> --> (token_id, tfidf score)\n",
    "'''\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 4.1 步：使用词袋运行 LDA ##\n",
    "\n",
    "我们将处理文档语料库中的 10 个话题。\n",
    "\n",
    "** 我们将使用所有 CPU 核心运行 LDA，以并行化并加快模型训练。**\n",
    "\n",
    "我们将调整以下参数：\n",
    "\n",
    "* **num_topics** 是请求从训练语料库中提取的潜在话题数。\n",
    "* **id2word** 是从单词 ID（整数）到单词（字符串）的映射，用于判断词汇表大小，以及用于调试和输出话题。\n",
    "* **workers** 是用于并行化的额外进程数。默认使用所有可用的核心。\n",
    "* **alpha** 和 **eta** 是影响文档-话题 (θ) 和话题-单词 (lambda) 分布的超参数。暂时使用默认值（默认值为 `1/num_topics`）\n",
    "    - Alpha 是文档-话题分布。\n",
    "        * alpha 很高：每个文档都包含所有话题（文档似乎都相似）。\n",
    "        * alpha 很低：每个文档包含的话题很少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eta 是话题-单词分布。\n",
    "    * eta 很高：每个话题都包含大部分单词（话题似乎都相似）。\n",
    "    * eta 很低：每个话题包含的单词很少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** 通过次数** 是通过语料库的训练次数。例如，如果训练语料库有 50,000 个文档，块大小是 10,000，通过次数是 2，则在线训练需要更新 10 次：\n",
    "    * `#1 documents 0-9,999 `\n",
    "    * `#2 documents 10,000-19,999 `\n",
    "    * `#3 documents 20,000-29,999 `\n",
    "    * `#4 documents 30,000-39,999 `\n",
    "    * `#5 documents 40,000-49,999 `\n",
    "    * `#6 documents 0-9,999 `\n",
    "    * `#7 documents 10,000-19,999 `\n",
    "    * `#8 documents 20,000-29,999 `\n",
    "    * `#9 documents 30,000-39,999 `\n",
    "    * `#10 documents 40,000-49,999`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "#                                    num_topics = 10, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 50)\n",
    "\n",
    "# LDA multicore \n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "# TODO\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=m_topics=10, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0.043*\"charg\" + 0.035*\"face\" + 0.032*\"court\" + 0.022*\"accus\" + 0.022*\"closer\" + 0.021*\"drug\" + 0.020*\"concern\" + 0.018*\"jail\" + 0.017*\"murder\" + 0.016*\"arrest\" \n",
      "Words: 0\n",
      "\n",
      "\n",
      "Topic: 0.036*\"govt\" + 0.027*\"plan\" + 0.023*\"water\" + 0.022*\"urg\" + 0.019*\"council\" + 0.014*\"fund\" + 0.012*\"boost\" + 0.012*\"health\" + 0.012*\"rise\" + 0.011*\"servic\" \n",
      "Words: 1\n",
      "\n",
      "\n",
      "Topic: 0.026*\"chang\" + 0.018*\"govt\" + 0.016*\"case\" + 0.016*\"fight\" + 0.015*\"help\" + 0.015*\"rule\" + 0.013*\"urg\" + 0.013*\"park\" + 0.012*\"law\" + 0.010*\"guilti\" \n",
      "Words: 2\n",
      "\n",
      "\n",
      "Topic: 0.020*\"miss\" + 0.019*\"search\" + 0.016*\"communiti\" + 0.016*\"look\" + 0.015*\"land\" + 0.014*\"begin\" + 0.013*\"hold\" + 0.013*\"releas\" + 0.013*\"elect\" + 0.011*\"crew\" \n",
      "Words: 3\n",
      "\n",
      "\n",
      "Topic: 0.021*\"drought\" + 0.018*\"year\" + 0.016*\"break\" + 0.015*\"expect\" + 0.014*\"busi\" + 0.012*\"farm\" + 0.011*\"power\" + 0.011*\"highway\" + 0.011*\"award\" + 0.010*\"confid\" \n",
      "Words: 4\n",
      "\n",
      "\n",
      "Topic: 0.024*\"school\" + 0.024*\"test\" + 0.016*\"south\" + 0.015*\"time\" + 0.015*\"centr\" + 0.013*\"head\" + 0.011*\"england\" + 0.011*\"say\" + 0.011*\"rudd\" + 0.010*\"control\" \n",
      "Words: 5\n",
      "\n",
      "\n",
      "Topic: 0.091*\"polic\" + 0.031*\"crash\" + 0.029*\"death\" + 0.025*\"investig\" + 0.015*\"die\" + 0.015*\"driver\" + 0.015*\"probe\" + 0.014*\"blaze\" + 0.014*\"road\" + 0.013*\"victim\" \n",
      "Words: 6\n",
      "\n",
      "\n",
      "Topic: 0.025*\"open\" + 0.016*\"coast\" + 0.014*\"dead\" + 0.013*\"hospit\" + 0.012*\"gold\" + 0.012*\"shoot\" + 0.011*\"injur\" + 0.010*\"world\" + 0.010*\"victori\" + 0.010*\"final\" \n",
      "Words: 7\n",
      "\n",
      "\n",
      "Topic: 0.021*\"australia\" + 0.017*\"forc\" + 0.014*\"water\" + 0.014*\"rain\" + 0.012*\"melbourn\" + 0.012*\"record\" + 0.012*\"threat\" + 0.011*\"terror\" + 0.011*\"lead\" + 0.010*\"return\" \n",
      "Words: 8\n",
      "\n",
      "\n",
      "Topic: 0.033*\"kill\" + 0.026*\"iraq\" + 0.024*\"say\" + 0.021*\"opposit\" + 0.016*\"attack\" + 0.015*\"market\" + 0.014*\"troop\" + 0.013*\"report\" + 0.012*\"sale\" + 0.011*\"target\" \n",
      "Words: 9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(topic, idx ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *** 主题分类 ***\n",
    "\n",
    "根据每个话题中的单词及其对应的权重，你能够推断出哪些类别？\n",
    "\n",
    "* 0: \n",
    "* 1: \n",
    "* 2: \n",
    "* 3: \n",
    "* 4: \n",
    "* 5: \n",
    "* 6: \n",
    "* 7:  \n",
    "* 8: \n",
    "* 9: \n",
    "\n",
    "## 第 4.2 步：使用 TF-IDF 运行 LDA ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Define lda model using corpus_tfidf, again using gensim.models.LdaMulticore()\n",
    "'''\n",
    "# TODO\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                             num_topics=10, \n",
    "                                             id2word = dictionary, \n",
    "                                             passes = 2, \n",
    "                                             workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.009*\"govt\" + 0.009*\"plan\" + 0.008*\"council\" + 0.006*\"rudd\" + 0.006*\"urg\" + 0.006*\"nation\" + 0.006*\"action\" + 0.006*\"chang\" + 0.006*\"reject\" + 0.005*\"call\"\n",
      "\n",
      "\n",
      "Topic: 1 Word: 0.009*\"open\" + 0.008*\"world\" + 0.008*\"hick\" + 0.007*\"market\" + 0.006*\"final\" + 0.006*\"australia\" + 0.005*\"aussi\" + 0.005*\"news\" + 0.005*\"kangaroo\" + 0.005*\"win\"\n",
      "\n",
      "\n",
      "Topic: 2 Word: 0.018*\"crash\" + 0.011*\"die\" + 0.009*\"toll\" + 0.008*\"road\" + 0.008*\"rise\" + 0.008*\"kill\" + 0.007*\"death\" + 0.007*\"plane\" + 0.007*\"rate\" + 0.006*\"victim\"\n",
      "\n",
      "\n",
      "Topic: 3 Word: 0.010*\"guilti\" + 0.007*\"nuclear\" + 0.007*\"plead\" + 0.007*\"grower\" + 0.006*\"iran\" + 0.006*\"timor\" + 0.005*\"season\" + 0.005*\"revamp\" + 0.005*\"coal\" + 0.005*\"confid\"\n",
      "\n",
      "\n",
      "Topic: 4 Word: 0.018*\"kill\" + 0.015*\"polic\" + 0.011*\"blaze\" + 0.010*\"iraq\" + 0.009*\"arrest\" + 0.009*\"attack\" + 0.009*\"firefight\" + 0.009*\"bomb\" + 0.008*\"charg\" + 0.008*\"troop\"\n",
      "\n",
      "\n",
      "Topic: 5 Word: 0.010*\"liber\" + 0.006*\"climat\" + 0.006*\"costello\" + 0.006*\"labor\" + 0.006*\"solomon\" + 0.006*\"grant\" + 0.006*\"speed\" + 0.005*\"lion\" + 0.005*\"post\" + 0.005*\"takeov\"\n",
      "\n",
      "\n",
      "Topic: 6 Word: 0.032*\"closer\" + 0.008*\"strike\" + 0.008*\"nurs\" + 0.008*\"south\" + 0.007*\"west\" + 0.006*\"uranium\" + 0.006*\"fish\" + 0.006*\"cyclon\" + 0.005*\"illeg\" + 0.005*\"murray\"\n",
      "\n",
      "\n",
      "Topic: 7 Word: 0.016*\"water\" + 0.014*\"govt\" + 0.012*\"fund\" + 0.010*\"council\" + 0.010*\"plan\" + 0.009*\"health\" + 0.009*\"urg\" + 0.007*\"boost\" + 0.006*\"drought\" + 0.006*\"servic\"\n",
      "\n",
      "\n",
      "Topic: 8 Word: 0.017*\"miss\" + 0.015*\"search\" + 0.014*\"polic\" + 0.011*\"murder\" + 0.010*\"court\" + 0.010*\"woman\" + 0.010*\"charg\" + 0.009*\"teen\" + 0.008*\"coast\" + 0.007*\"fatal\"\n",
      "\n",
      "\n",
      "Topic: 9 Word: 0.011*\"break\" + 0.007*\"bird\" + 0.007*\"hill\" + 0.007*\"burn\" + 0.006*\"refus\" + 0.005*\"properti\" + 0.005*\"spend\" + 0.005*\"warrior\" + 0.004*\"plant\" + 0.004*\"govt\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *** 主题分类 *** \n",
    "\n",
    "可以看出，在使用 tf-idf 时，不太常见的单词权重更高，导致名词被考虑在内。这样就更难分类，因为名词比较难分类。进一步表明我们应用的模型取决于要处理的文本语料库的类型。\n",
    "\n",
    "根据每个话题中的单词及其对应的权重，你能够推断出哪些类别？\n",
    "\n",
    "* 0: \n",
    "* 1:  \n",
    "* 2: \n",
    "* 3: \n",
    "* 4:  \n",
    "* 5: \n",
    "* 6: \n",
    "* 7: \n",
    "* 8: \n",
    "* 9: \n",
    "\n",
    "## 第 5.1 步：LDA 词袋模型评估性能##\n",
    "- 分类样本文档\n",
    "\n",
    "我们将检查可以在何处分类测试文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain', 'help', 'dampen', 'bushfir']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Text of sample document 4310\n",
    "'''\n",
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8199663758277893\t \n",
      "Topic: 0.021*\"australia\" + 0.017*\"forc\" + 0.014*\"water\" + 0.014*\"rain\" + 0.012*\"melbourn\" + 0.012*\"record\" + 0.012*\"threat\" + 0.011*\"terror\" + 0.011*\"lead\" + 0.010*\"return\"\n",
      "\n",
      "Score: 0.02001069486141205\t \n",
      "Topic: 0.026*\"chang\" + 0.018*\"govt\" + 0.016*\"case\" + 0.016*\"fight\" + 0.015*\"help\" + 0.015*\"rule\" + 0.013*\"urg\" + 0.013*\"park\" + 0.012*\"law\" + 0.010*\"guilti\"\n",
      "\n",
      "Score: 0.020006701350212097\t \n",
      "Topic: 0.020*\"miss\" + 0.019*\"search\" + 0.016*\"communiti\" + 0.016*\"look\" + 0.015*\"land\" + 0.014*\"begin\" + 0.013*\"hold\" + 0.013*\"releas\" + 0.013*\"elect\" + 0.011*\"crew\"\n",
      "\n",
      "Score: 0.020004503428936005\t \n",
      "Topic: 0.091*\"polic\" + 0.031*\"crash\" + 0.029*\"death\" + 0.025*\"investig\" + 0.015*\"die\" + 0.015*\"driver\" + 0.015*\"probe\" + 0.014*\"blaze\" + 0.014*\"road\" + 0.013*\"victim\"\n",
      "\n",
      "Score: 0.020002929493784904\t \n",
      "Topic: 0.021*\"drought\" + 0.018*\"year\" + 0.016*\"break\" + 0.015*\"expect\" + 0.014*\"busi\" + 0.012*\"farm\" + 0.011*\"power\" + 0.011*\"highway\" + 0.011*\"award\" + 0.010*\"confid\"\n",
      "\n",
      "Score: 0.020002353936433792\t \n",
      "Topic: 0.033*\"kill\" + 0.026*\"iraq\" + 0.024*\"say\" + 0.021*\"opposit\" + 0.016*\"attack\" + 0.015*\"market\" + 0.014*\"troop\" + 0.013*\"report\" + 0.012*\"sale\" + 0.011*\"target\"\n",
      "\n",
      "Score: 0.02000163123011589\t \n",
      "Topic: 0.036*\"govt\" + 0.027*\"plan\" + 0.023*\"water\" + 0.022*\"urg\" + 0.019*\"council\" + 0.014*\"fund\" + 0.012*\"boost\" + 0.012*\"health\" + 0.012*\"rise\" + 0.011*\"servic\"\n",
      "\n",
      "Score: 0.020001616328954697\t \n",
      "Topic: 0.024*\"school\" + 0.024*\"test\" + 0.016*\"south\" + 0.015*\"time\" + 0.015*\"centr\" + 0.013*\"head\" + 0.011*\"england\" + 0.011*\"say\" + 0.011*\"rudd\" + 0.010*\"control\"\n",
      "\n",
      "Score: 0.020001616328954697\t \n",
      "Topic: 0.025*\"open\" + 0.016*\"coast\" + 0.014*\"dead\" + 0.013*\"hospit\" + 0.012*\"gold\" + 0.012*\"shoot\" + 0.011*\"injur\" + 0.010*\"world\" + 0.010*\"victori\" + 0.010*\"final\"\n",
      "\n",
      "Score: 0.020001573488116264\t \n",
      "Topic: 0.043*\"charg\" + 0.035*\"face\" + 0.032*\"court\" + 0.022*\"accus\" + 0.022*\"closer\" + 0.021*\"drug\" + 0.020*\"concern\" + 0.018*\"jail\" + 0.017*\"murder\" + 0.016*\"arrest\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check which topic our test document belongs to using the LDA Bag of Words model.\n",
    "'''\n",
    "document_num = 4310\n",
    "# Our test document is document number 4310\n",
    "\n",
    "# TODO\n",
    "# Our test document is document number 4310\n",
    "for index, score in sorted(lda_model[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "它成为我们所分配话题（X，分类正确）的一部分的概率最高\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 5.2 步：LDA TF-IDF 模型评估性能##\n",
    "- 分类样本文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4492030441761017\t \n",
      "Topic: 0.016*\"water\" + 0.014*\"govt\" + 0.012*\"fund\" + 0.010*\"council\" + 0.010*\"plan\" + 0.009*\"health\" + 0.009*\"urg\" + 0.007*\"boost\" + 0.006*\"drought\" + 0.006*\"servic\"\n",
      "\n",
      "Score: 0.3907369077205658\t \n",
      "Topic: 0.009*\"govt\" + 0.009*\"plan\" + 0.008*\"council\" + 0.006*\"rudd\" + 0.006*\"urg\" + 0.006*\"nation\" + 0.006*\"action\" + 0.006*\"chang\" + 0.006*\"reject\" + 0.005*\"call\"\n",
      "\n",
      "Score: 0.02001015469431877\t \n",
      "Topic: 0.018*\"crash\" + 0.011*\"die\" + 0.009*\"toll\" + 0.008*\"road\" + 0.008*\"rise\" + 0.008*\"kill\" + 0.007*\"death\" + 0.007*\"plane\" + 0.007*\"rate\" + 0.006*\"victim\"\n",
      "\n",
      "Score: 0.020008662715554237\t \n",
      "Topic: 0.010*\"guilti\" + 0.007*\"nuclear\" + 0.007*\"plead\" + 0.007*\"grower\" + 0.006*\"iran\" + 0.006*\"timor\" + 0.005*\"season\" + 0.005*\"revamp\" + 0.005*\"coal\" + 0.005*\"confid\"\n",
      "\n",
      "Score: 0.020007897168397903\t \n",
      "Topic: 0.017*\"miss\" + 0.015*\"search\" + 0.014*\"polic\" + 0.011*\"murder\" + 0.010*\"court\" + 0.010*\"woman\" + 0.010*\"charg\" + 0.009*\"teen\" + 0.008*\"coast\" + 0.007*\"fatal\"\n",
      "\n",
      "Score: 0.020007535815238953\t \n",
      "Topic: 0.018*\"kill\" + 0.015*\"polic\" + 0.011*\"blaze\" + 0.010*\"iraq\" + 0.009*\"arrest\" + 0.009*\"attack\" + 0.009*\"firefight\" + 0.009*\"bomb\" + 0.008*\"charg\" + 0.008*\"troop\"\n",
      "\n",
      "Score: 0.02000744827091694\t \n",
      "Topic: 0.032*\"closer\" + 0.008*\"strike\" + 0.008*\"nurs\" + 0.008*\"south\" + 0.007*\"west\" + 0.006*\"uranium\" + 0.006*\"fish\" + 0.006*\"cyclon\" + 0.005*\"illeg\" + 0.005*\"murray\"\n",
      "\n",
      "Score: 0.02000742219388485\t \n",
      "Topic: 0.011*\"break\" + 0.007*\"bird\" + 0.007*\"hill\" + 0.007*\"burn\" + 0.006*\"refus\" + 0.005*\"properti\" + 0.005*\"spend\" + 0.005*\"warrior\" + 0.004*\"plant\" + 0.004*\"govt\"\n",
      "\n",
      "Score: 0.02000616490840912\t \n",
      "Topic: 0.009*\"open\" + 0.008*\"world\" + 0.008*\"hick\" + 0.007*\"market\" + 0.006*\"final\" + 0.006*\"australia\" + 0.005*\"aussi\" + 0.005*\"news\" + 0.005*\"kangaroo\" + 0.005*\"win\"\n",
      "\n",
      "Score: 0.02000477910041809\t \n",
      "Topic: 0.010*\"liber\" + 0.006*\"climat\" + 0.006*\"costello\" + 0.006*\"labor\" + 0.006*\"solomon\" + 0.006*\"grant\" + 0.006*\"speed\" + 0.005*\"lion\" + 0.005*\"post\" + 0.005*\"takeov\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check which topic our test document belongs to using the LDA TF-IDF model.\n",
    "'''\n",
    "# Our test document is document number 4310\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "它成为我们所分配话题 (X) 的一部分的概率最高 (`x%`)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 6 步：用未使用文档测试LDA词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6200637817382812\t Topic: 0.021*\"drought\" + 0.018*\"year\" + 0.016*\"break\" + 0.015*\"expect\" + 0.014*\"busi\"\n",
      "Score: 0.21990899741649628\t Topic: 0.026*\"chang\" + 0.018*\"govt\" + 0.016*\"case\" + 0.016*\"fight\" + 0.015*\"help\"\n",
      "Score: 0.020003952085971832\t Topic: 0.025*\"open\" + 0.016*\"coast\" + 0.014*\"dead\" + 0.013*\"hospit\" + 0.012*\"gold\"\n",
      "Score: 0.020003316923975945\t Topic: 0.043*\"charg\" + 0.035*\"face\" + 0.032*\"court\" + 0.022*\"accus\" + 0.022*\"closer\"\n",
      "Score: 0.020003316923975945\t Topic: 0.036*\"govt\" + 0.027*\"plan\" + 0.023*\"water\" + 0.022*\"urg\" + 0.019*\"council\"\n",
      "Score: 0.020003316923975945\t Topic: 0.020*\"miss\" + 0.019*\"search\" + 0.016*\"communiti\" + 0.016*\"look\" + 0.015*\"land\"\n",
      "Score: 0.020003316923975945\t Topic: 0.024*\"school\" + 0.024*\"test\" + 0.016*\"south\" + 0.015*\"time\" + 0.015*\"centr\"\n",
      "Score: 0.020003316923975945\t Topic: 0.091*\"polic\" + 0.031*\"crash\" + 0.029*\"death\" + 0.025*\"investig\" + 0.015*\"die\"\n",
      "Score: 0.020003316923975945\t Topic: 0.021*\"australia\" + 0.017*\"forc\" + 0.014*\"water\" + 0.014*\"rain\" + 0.012*\"melbourn\"\n",
      "Score: 0.020003316923975945\t Topic: 0.033*\"kill\" + 0.026*\"iraq\" + 0.024*\"say\" + 0.021*\"opposit\" + 0.016*\"attack\"\n"
     ]
    }
   ],
   "source": [
    "# lda_model_tfidf -- lda_model\n",
    "unseen_document = \"My favorite sports activities are running and swimming.\"\n",
    "\n",
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "模型正确地将未见过的文档分类成 X 类别，概率是 x%。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__END__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
