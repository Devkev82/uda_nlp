{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test FFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sinusoid(freq):\n",
    "    \"\"\"\n",
    "    return a sinusoidal of random amplitude and phase for a given frequency\n",
    "    :param freq: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    phase = np.random.random()\n",
    "    amplitude = 2 * (np.random.random_integers(1, 10))\n",
    "    return amplitude * np.cos(2 * np.pi * freq - phase)\n",
    "\n",
    "\n",
    "def get_wave_timing(num_samples=500, range_of_time = 5.0):\n",
    "    \"\"\"\n",
    "    provide an array of time values of size num_samples spread evenly over range_of_time\n",
    "    :param num_samples: int \n",
    "    :param range_of_time: float\n",
    "    :return: int, float, np.array\n",
    "    \"\"\"\n",
    "    # sample spacing\n",
    "    spacing = range_of_time / num_samples\n",
    "    # array for time samples\n",
    "    t = np.linspace(0.0, range_of_time, num_samples)\n",
    "    return num_samples, spacing, t\n",
    "\n",
    "\n",
    "def make_waves(t, freqs):\n",
    "    \"\"\"\n",
    "    convert three frequencies into arrays of discrete values representing sinusoidal waves\n",
    "    :param freqs: [float, float, float]\n",
    "    :return: [np.array, np.array, np.array]\n",
    "    \"\"\"\n",
    "    w0 = sinusoid(t * freqs[0])\n",
    "    w1 = sinusoid(t * freqs[1])\n",
    "    w2 = sinusoid(t * freqs[2])\n",
    "    return w0, w1, w2\n",
    "\n",
    "\n",
    "def display_sinusoids(time_array, f1, f2, f3, sum):\n",
    "    # plot three frequencies with random phase shifts on y axis\n",
    "    # plt.figure()\n",
    "    fig, ax = plt.subplots(4, 1)\n",
    "    # plt.subplot(411)  # 3 rows, 1 column, fignum 1\n",
    "    ax[0].plot(time_array, f1)\n",
    "    ax[0].set_title('1st frequency component')\n",
    "\n",
    "    # plt.subplot(412)  # 3 rows, 1 column, fignum 2\n",
    "    ax[1].plot(time_array, f2)\n",
    "    ax[1].set_title('2nd frequency component')\n",
    "\n",
    "    # plt.subplot(413)  # 3 rows, 1 column, fignum 3\n",
    "    ax[2].plot(time_array, f3)\n",
    "    ax[2].set_title('3rd frequency component')\n",
    "\n",
    "    # sum\n",
    "    # plt.subplot(414)  # 3 rows, 1 column, fignum 4\n",
    "    ax[3].plot(time_array, sum, 'r')\n",
    "    ax[3].set_title('Sum of components')\n",
    "    ax[3].set_ylabel('amplitude')\n",
    "    ax[3].set_xlabel('time')\n",
    "\n",
    "    # adjust format of display to make room for titles\n",
    "    plt.subplots_adjust(\n",
    "        top=0.94,\n",
    "        bottom=0.11,\n",
    "        left=0.11,\n",
    "        right=0.97,\n",
    "        hspace=0.65,\n",
    "        wspace=0.2\n",
    "    )\n",
    "    # plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def display_fft(xf, yf):\n",
    "    num_samples = np.shape(yf)[0]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xf, 2.0 / num_samples * np.abs(yf[:num_samples // 2]))\n",
    "    plt.title('Fast Fourier Transform')\n",
    "    plt.xlabel('frequency')\n",
    "    plt.ylabel('amplitude')\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function.py\n",
    "import numpy as np\n",
    "import scipy.fftpack\n",
    "\n",
    "def choose_frequencies():\n",
    "    \"\"\"\n",
    "    # provide three frequencies in a range between 1 and 50    \n",
    "    :return: [int, int, int]\n",
    "    \"\"\"\n",
    "    # *** TODO provide three frequencies between 1 and 50\n",
    "    freq1 = 1\n",
    "    freq2 = 5\n",
    "    freq3 = 25\n",
    "    # end TODO\n",
    "\n",
    "    return [freq1, freq2, freq3]\n",
    "\n",
    "\n",
    "def add_the_waves(freqs):\n",
    "    \"\"\"\n",
    "    create three sinusoidal waves and one wave that is the sum of the three\n",
    "    :param freqs: [int, int, int]\n",
    "    :return: [np.array, np.array, np.array, np.array]\n",
    "        representing wave1, wave2, wave3, sum of waves\n",
    "        each array contains 500(by default) discrete values for plotting a sinusoidal\n",
    "    \"\"\"\n",
    "    _, _, t = get_wave_timing()\n",
    "    w1, w2, w3 = make_waves(t, freqs)\n",
    "\n",
    "    # TODO sum the waves together to form sum_waves\n",
    "    sum_waves = w1+w2+w3\n",
    "    # end TODO\n",
    "\n",
    "    return [w1, w2, w3, sum_waves]\n",
    "\n",
    "\n",
    "def demo_fft(sum_waves):\n",
    "    num_samples, spacing, _ = get_wave_timing()\n",
    "\n",
    "    # TODO create a Fast Fourier Transform of the waveform using scipy.fftpack.fft\n",
    "    # named 'y_fft'\n",
    "    y_fft = scipy.fftpack.fft(sum_waves)\n",
    "    # end TODO\n",
    "\n",
    "    x_fft = np.linspace(0.0, 1.0/spacing, num_samples)\n",
    "    return x_fft, y_fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fft in module scipy.fftpack.basic:\n",
      "\n",
      "fft(x, n=None, axis=-1, overwrite_x=False)\n",
      "    Return discrete Fourier transform of real or complex sequence.\n",
      "    \n",
      "    The returned complex array contains ``y(0), y(1),..., y(n-1)`` where\n",
      "    \n",
      "    ``y(j) = (x * exp(-2*pi*sqrt(-1)*j*np.arange(n)/n)).sum()``.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x : array_like\n",
      "        Array to Fourier transform.\n",
      "    n : int, optional\n",
      "        Length of the Fourier transform.  If ``n < x.shape[axis]``, `x` is\n",
      "        truncated.  If ``n > x.shape[axis]``, `x` is zero-padded. The\n",
      "        default results in ``n = x.shape[axis]``.\n",
      "    axis : int, optional\n",
      "        Axis along which the fft's are computed; the default is over the\n",
      "        last axis (i.e., ``axis=-1``).\n",
      "    overwrite_x : bool, optional\n",
      "        If True, the contents of `x` can be destroyed; the default is False.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    z : complex ndarray\n",
      "        with the elements::\n",
      "    \n",
      "            [y(0),y(1),..,y(n/2),y(1-n/2),...,y(-1)]        if n is even\n",
      "            [y(0),y(1),..,y((n-1)/2),y(-(n-1)/2),...,y(-1)]  if n is odd\n",
      "    \n",
      "        where::\n",
      "    \n",
      "            y(j) = sum[k=0..n-1] x[k] * exp(-sqrt(-1)*j*k* 2*pi/n), j = 0..n-1\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    ifft : Inverse FFT\n",
      "    rfft : FFT of a real sequence\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The packing of the result is \"standard\": If ``A = fft(a, n)``, then\n",
      "    ``A[0]`` contains the zero-frequency term, ``A[1:n/2]`` contains the\n",
      "    positive-frequency terms, and ``A[n/2:]`` contains the negative-frequency\n",
      "    terms, in order of decreasingly negative frequency. So for an 8-point\n",
      "    transform, the frequencies of the result are [0, 1, 2, 3, -4, -3, -2, -1].\n",
      "    To rearrange the fft output so that the zero-frequency component is\n",
      "    centered, like [-4, -3, -2, -1,  0,  1,  2,  3], use `fftshift`.\n",
      "    \n",
      "    Both single and double precision routines are implemented.  Half precision\n",
      "    inputs will be converted to single precision.  Non floating-point inputs\n",
      "    will be converted to double precision.  Long-double precision inputs are\n",
      "    not supported.\n",
      "    \n",
      "    This function is most efficient when `n` is a power of two, and least\n",
      "    efficient when `n` is prime.\n",
      "    \n",
      "    Note that if ``x`` is real-valued then ``A[j] == A[n-j].conjugate()``.\n",
      "    If ``x`` is real-valued and ``n`` is even then ``A[n/2]`` is real.\n",
      "    \n",
      "    If the data type of `x` is real, a \"real FFT\" algorithm is automatically\n",
      "    used, which roughly halves the computation time.  To increase efficiency\n",
      "    a little further, use `rfft`, which does the same calculation, but only\n",
      "    outputs half of the symmetrical spectrum.  If the data is both real and\n",
      "    symmetrical, the `dct` can again double the efficiency, by generating\n",
      "    half of the spectrum from half of the signal.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from scipy.fftpack import fft, ifft\n",
      "    >>> x = np.arange(5)\n",
      "    >>> np.allclose(fft(ifft(x)), x, atol=1e-15)  # within numerical accuracy.\n",
      "    True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.fftpack import fft\n",
    "\n",
    "help(fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.+0.j])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fft(np.array([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function linspace in module numpy:\n",
      "\n",
      "linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0)\n",
      "    Return evenly spaced numbers over a specified interval.\n",
      "    \n",
      "    Returns `num` evenly spaced samples, calculated over the\n",
      "    interval [`start`, `stop`].\n",
      "    \n",
      "    The endpoint of the interval can optionally be excluded.\n",
      "    \n",
      "    .. versionchanged:: 1.16.0\n",
      "        Non-scalar `start` and `stop` are now supported.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    start : array_like\n",
      "        The starting value of the sequence.\n",
      "    stop : array_like\n",
      "        The end value of the sequence, unless `endpoint` is set to False.\n",
      "        In that case, the sequence consists of all but the last of ``num + 1``\n",
      "        evenly spaced samples, so that `stop` is excluded.  Note that the step\n",
      "        size changes when `endpoint` is False.\n",
      "    num : int, optional\n",
      "        Number of samples to generate. Default is 50. Must be non-negative.\n",
      "    endpoint : bool, optional\n",
      "        If True, `stop` is the last sample. Otherwise, it is not included.\n",
      "        Default is True.\n",
      "    retstep : bool, optional\n",
      "        If True, return (`samples`, `step`), where `step` is the spacing\n",
      "        between samples.\n",
      "    dtype : dtype, optional\n",
      "        The type of the output array.  If `dtype` is not given, infer the data\n",
      "        type from the other input arguments.\n",
      "    \n",
      "        .. versionadded:: 1.9.0\n",
      "    \n",
      "    axis : int, optional\n",
      "        The axis in the result to store the samples.  Relevant only if start\n",
      "        or stop are array-like.  By default (0), the samples will be along a\n",
      "        new axis inserted at the beginning. Use -1 to get an axis at the end.\n",
      "    \n",
      "        .. versionadded:: 1.16.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    samples : ndarray\n",
      "        There are `num` equally spaced samples in the closed interval\n",
      "        ``[start, stop]`` or the half-open interval ``[start, stop)``\n",
      "        (depending on whether `endpoint` is True or False).\n",
      "    step : float, optional\n",
      "        Only returned if `retstep` is True\n",
      "    \n",
      "        Size of spacing between samples.\n",
      "    \n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    arange : Similar to `linspace`, but uses a step size (instead of the\n",
      "             number of samples).\n",
      "    geomspace : Similar to `linspace`, but with numbers spaced evenly on a log\n",
      "                scale (a geometric progression).\n",
      "    logspace : Similar to `geomspace`, but with the end points specified as\n",
      "               logarithms.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.linspace(2.0, 3.0, num=5)\n",
      "    array([2.  , 2.25, 2.5 , 2.75, 3.  ])\n",
      "    >>> np.linspace(2.0, 3.0, num=5, endpoint=False)\n",
      "    array([2. ,  2.2,  2.4,  2.6,  2.8])\n",
      "    >>> np.linspace(2.0, 3.0, num=5, retstep=True)\n",
      "    (array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n",
      "    \n",
      "    Graphical illustration:\n",
      "    \n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> N = 8\n",
      "    >>> y = np.zeros(N)\n",
      "    >>> x1 = np.linspace(0, 10, N, endpoint=True)\n",
      "    >>> x2 = np.linspace(0, 10, N, endpoint=False)\n",
      "    >>> plt.plot(x1, y, 'o')\n",
      "    [<matplotlib.lines.Line2D object at 0x...>]\n",
      "    >>> plt.plot(x2, y + 0.5, 'o')\n",
      "    [<matplotlib.lines.Line2D object at 0x...>]\n",
      "    >>> plt.ylim([-0.5, 1])\n",
      "    (-0.5, 1)\n",
      "    >>> plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.linspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   0.2004008 ,   0.4008016 ,   0.6012024 ,\n",
       "         0.80160321,   1.00200401,   1.20240481,   1.40280561,\n",
       "         1.60320641,   1.80360721,   2.00400802,   2.20440882,\n",
       "         2.40480962,   2.60521042,   2.80561122,   3.00601202,\n",
       "         3.20641283,   3.40681363,   3.60721443,   3.80761523,\n",
       "         4.00801603,   4.20841683,   4.40881764,   4.60921844,\n",
       "         4.80961924,   5.01002004,   5.21042084,   5.41082164,\n",
       "         5.61122244,   5.81162325,   6.01202405,   6.21242485,\n",
       "         6.41282565,   6.61322645,   6.81362725,   7.01402806,\n",
       "         7.21442886,   7.41482966,   7.61523046,   7.81563126,\n",
       "         8.01603206,   8.21643287,   8.41683367,   8.61723447,\n",
       "         8.81763527,   9.01803607,   9.21843687,   9.41883768,\n",
       "         9.61923848,   9.81963928,  10.02004008,  10.22044088,\n",
       "        10.42084168,  10.62124248,  10.82164329,  11.02204409,\n",
       "        11.22244489,  11.42284569,  11.62324649,  11.82364729,\n",
       "        12.0240481 ,  12.2244489 ,  12.4248497 ,  12.6252505 ,\n",
       "        12.8256513 ,  13.0260521 ,  13.22645291,  13.42685371,\n",
       "        13.62725451,  13.82765531,  14.02805611,  14.22845691,\n",
       "        14.42885772,  14.62925852,  14.82965932,  15.03006012,\n",
       "        15.23046092,  15.43086172,  15.63126253,  15.83166333,\n",
       "        16.03206413,  16.23246493,  16.43286573,  16.63326653,\n",
       "        16.83366733,  17.03406814,  17.23446894,  17.43486974,\n",
       "        17.63527054,  17.83567134,  18.03607214,  18.23647295,\n",
       "        18.43687375,  18.63727455,  18.83767535,  19.03807615,\n",
       "        19.23847695,  19.43887776,  19.63927856,  19.83967936,\n",
       "        20.04008016,  20.24048096,  20.44088176,  20.64128257,\n",
       "        20.84168337,  21.04208417,  21.24248497,  21.44288577,\n",
       "        21.64328657,  21.84368737,  22.04408818,  22.24448898,\n",
       "        22.44488978,  22.64529058,  22.84569138,  23.04609218,\n",
       "        23.24649299,  23.44689379,  23.64729459,  23.84769539,\n",
       "        24.04809619,  24.24849699,  24.4488978 ,  24.6492986 ,\n",
       "        24.8496994 ,  25.0501002 ,  25.250501  ,  25.4509018 ,\n",
       "        25.65130261,  25.85170341,  26.05210421,  26.25250501,\n",
       "        26.45290581,  26.65330661,  26.85370741,  27.05410822,\n",
       "        27.25450902,  27.45490982,  27.65531062,  27.85571142,\n",
       "        28.05611222,  28.25651303,  28.45691383,  28.65731463,\n",
       "        28.85771543,  29.05811623,  29.25851703,  29.45891784,\n",
       "        29.65931864,  29.85971944,  30.06012024,  30.26052104,\n",
       "        30.46092184,  30.66132265,  30.86172345,  31.06212425,\n",
       "        31.26252505,  31.46292585,  31.66332665,  31.86372745,\n",
       "        32.06412826,  32.26452906,  32.46492986,  32.66533066,\n",
       "        32.86573146,  33.06613226,  33.26653307,  33.46693387,\n",
       "        33.66733467,  33.86773547,  34.06813627,  34.26853707,\n",
       "        34.46893788,  34.66933868,  34.86973948,  35.07014028,\n",
       "        35.27054108,  35.47094188,  35.67134269,  35.87174349,\n",
       "        36.07214429,  36.27254509,  36.47294589,  36.67334669,\n",
       "        36.87374749,  37.0741483 ,  37.2745491 ,  37.4749499 ,\n",
       "        37.6753507 ,  37.8757515 ,  38.0761523 ,  38.27655311,\n",
       "        38.47695391,  38.67735471,  38.87775551,  39.07815631,\n",
       "        39.27855711,  39.47895792,  39.67935872,  39.87975952,\n",
       "        40.08016032,  40.28056112,  40.48096192,  40.68136273,\n",
       "        40.88176353,  41.08216433,  41.28256513,  41.48296593,\n",
       "        41.68336673,  41.88376754,  42.08416834,  42.28456914,\n",
       "        42.48496994,  42.68537074,  42.88577154,  43.08617234,\n",
       "        43.28657315,  43.48697395,  43.68737475,  43.88777555,\n",
       "        44.08817635,  44.28857715,  44.48897796,  44.68937876,\n",
       "        44.88977956,  45.09018036,  45.29058116,  45.49098196,\n",
       "        45.69138277,  45.89178357,  46.09218437,  46.29258517,\n",
       "        46.49298597,  46.69338677,  46.89378758,  47.09418838,\n",
       "        47.29458918,  47.49498998,  47.69539078,  47.89579158,\n",
       "        48.09619238,  48.29659319,  48.49699399,  48.69739479,\n",
       "        48.89779559,  49.09819639,  49.29859719,  49.498998  ,\n",
       "        49.6993988 ,  49.8997996 ,  50.1002004 ,  50.3006012 ,\n",
       "        50.501002  ,  50.70140281,  50.90180361,  51.10220441,\n",
       "        51.30260521,  51.50300601,  51.70340681,  51.90380762,\n",
       "        52.10420842,  52.30460922,  52.50501002,  52.70541082,\n",
       "        52.90581162,  53.10621242,  53.30661323,  53.50701403,\n",
       "        53.70741483,  53.90781563,  54.10821643,  54.30861723,\n",
       "        54.50901804,  54.70941884,  54.90981964,  55.11022044,\n",
       "        55.31062124,  55.51102204,  55.71142285,  55.91182365,\n",
       "        56.11222445,  56.31262525,  56.51302605,  56.71342685,\n",
       "        56.91382766,  57.11422846,  57.31462926,  57.51503006,\n",
       "        57.71543086,  57.91583166,  58.11623246,  58.31663327,\n",
       "        58.51703407,  58.71743487,  58.91783567,  59.11823647,\n",
       "        59.31863727,  59.51903808,  59.71943888,  59.91983968,\n",
       "        60.12024048,  60.32064128,  60.52104208,  60.72144289,\n",
       "        60.92184369,  61.12224449,  61.32264529,  61.52304609,\n",
       "        61.72344689,  61.9238477 ,  62.1242485 ,  62.3246493 ,\n",
       "        62.5250501 ,  62.7254509 ,  62.9258517 ,  63.12625251,\n",
       "        63.32665331,  63.52705411,  63.72745491,  63.92785571,\n",
       "        64.12825651,  64.32865731,  64.52905812,  64.72945892,\n",
       "        64.92985972,  65.13026052,  65.33066132,  65.53106212,\n",
       "        65.73146293,  65.93186373,  66.13226453,  66.33266533,\n",
       "        66.53306613,  66.73346693,  66.93386774,  67.13426854,\n",
       "        67.33466934,  67.53507014,  67.73547094,  67.93587174,\n",
       "        68.13627255,  68.33667335,  68.53707415,  68.73747495,\n",
       "        68.93787575,  69.13827655,  69.33867735,  69.53907816,\n",
       "        69.73947896,  69.93987976,  70.14028056,  70.34068136,\n",
       "        70.54108216,  70.74148297,  70.94188377,  71.14228457,\n",
       "        71.34268537,  71.54308617,  71.74348697,  71.94388778,\n",
       "        72.14428858,  72.34468938,  72.54509018,  72.74549098,\n",
       "        72.94589178,  73.14629259,  73.34669339,  73.54709419,\n",
       "        73.74749499,  73.94789579,  74.14829659,  74.34869739,\n",
       "        74.5490982 ,  74.749499  ,  74.9498998 ,  75.1503006 ,\n",
       "        75.3507014 ,  75.5511022 ,  75.75150301,  75.95190381,\n",
       "        76.15230461,  76.35270541,  76.55310621,  76.75350701,\n",
       "        76.95390782,  77.15430862,  77.35470942,  77.55511022,\n",
       "        77.75551102,  77.95591182,  78.15631263,  78.35671343,\n",
       "        78.55711423,  78.75751503,  78.95791583,  79.15831663,\n",
       "        79.35871743,  79.55911824,  79.75951904,  79.95991984,\n",
       "        80.16032064,  80.36072144,  80.56112224,  80.76152305,\n",
       "        80.96192385,  81.16232465,  81.36272545,  81.56312625,\n",
       "        81.76352705,  81.96392786,  82.16432866,  82.36472946,\n",
       "        82.56513026,  82.76553106,  82.96593186,  83.16633267,\n",
       "        83.36673347,  83.56713427,  83.76753507,  83.96793587,\n",
       "        84.16833667,  84.36873747,  84.56913828,  84.76953908,\n",
       "        84.96993988,  85.17034068,  85.37074148,  85.57114228,\n",
       "        85.77154309,  85.97194389,  86.17234469,  86.37274549,\n",
       "        86.57314629,  86.77354709,  86.9739479 ,  87.1743487 ,\n",
       "        87.3747495 ,  87.5751503 ,  87.7755511 ,  87.9759519 ,\n",
       "        88.17635271,  88.37675351,  88.57715431,  88.77755511,\n",
       "        88.97795591,  89.17835671,  89.37875752,  89.57915832,\n",
       "        89.77955912,  89.97995992,  90.18036072,  90.38076152,\n",
       "        90.58116232,  90.78156313,  90.98196393,  91.18236473,\n",
       "        91.38276553,  91.58316633,  91.78356713,  91.98396794,\n",
       "        92.18436874,  92.38476954,  92.58517034,  92.78557114,\n",
       "        92.98597194,  93.18637275,  93.38677355,  93.58717435,\n",
       "        93.78757515,  93.98797595,  94.18837675,  94.38877756,\n",
       "        94.58917836,  94.78957916,  94.98997996,  95.19038076,\n",
       "        95.39078156,  95.59118236,  95.79158317,  95.99198397,\n",
       "        96.19238477,  96.39278557,  96.59318637,  96.79358717,\n",
       "        96.99398798,  97.19438878,  97.39478958,  97.59519038,\n",
       "        97.79559118,  97.99599198,  98.19639279,  98.39679359,\n",
       "        98.59719439,  98.79759519,  98.99799599,  99.19839679,\n",
       "        99.3987976 ,  99.5991984 ,  99.7995992 , 100.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.0, 1.0/(5.0/500), 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 .wav 文化转换为 MFCC 特征的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function.py\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "\n",
    "def wav_to_mfcc(wav_filename, num_cepstrum):\n",
    "    \"\"\" extract MFCC features from a wav file\n",
    "\n",
    "    :param wav_filename: filename with .wav format\n",
    "    :param num_cepstrum: number of cepstrum to return\n",
    "    :return: MFCC features for wav file\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO implement\n",
    "    (rate, sig) = wav.read(wav_filename)\n",
    "    mfcc_features = mfcc(sig, rate, numcep=num_cepstrum)\n",
    "    return mfcc_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test1 bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', 'i', 'love', 'language', 'models', '</s>']\n",
      "[('</s>', 'i'), ('i', 'love'), ('love', 'language'), ('language', 'models'), ('models', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'the old man spoke to me',\n",
    "    'me to spoke man old the',\n",
    "    'old man me old man me',\n",
    "]\n",
    "\n",
    "def sentence_to_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Add start '<s>' and stop '</s>' tags to the sentence and tokenize it into a list\n",
    "    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)\n",
    "    :param sentence: string\n",
    "    :return: list, list\n",
    "        sentence_tokens: ordered list of words found in the sentence\n",
    "        sentence_bigrams: a list of ordered two-word tuples found in the sentence\n",
    "    \"\"\"\n",
    "    #TODO implement\n",
    "    tag = '</s>'\n",
    "    sentence_tokens = [tag] + sentence.lower().split() + [tag]\n",
    "    sentence_bigrams = [(sentence_tokens[i], sentence_tokens[i+1]) for i in range(len(sentence_tokens)-1)]\n",
    "    return sentence_tokens, sentence_bigrams\n",
    "\n",
    "\n",
    "tokens1, bigram1 = sentence_to_bigrams('I love language models')\n",
    "\n",
    "print(tokens1)\n",
    "print(bigram1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'i', 'love', 'language', 'models', '</s>']\n",
      "[('<s>', 'i'), ('i', 'love'), ('love', 'language'), ('language', 'models'), ('models', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'the old man spoke to me',\n",
    "    'me to spoke man old the',\n",
    "    'old man me old man me',\n",
    "]\n",
    "\n",
    "def sentence_to_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Add start '<s>' and stop '</s>' tags to the sentence and tokenize it into a list\n",
    "    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)\n",
    "    :param sentence: string\n",
    "    :return: list, list\n",
    "        sentence_tokens: ordered list of words found in the sentence\n",
    "        sentence_bigrams: a list of ordered two-word tuples found in the sentence\n",
    "    \"\"\"\n",
    "    sentence_tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
    "    sentence_bigrams = []\n",
    "    for i in range(len(sentence_tokens)-1):\n",
    "        sentence_bigrams.append((sentence_tokens[i], sentence_tokens[i+1]))\n",
    "    return sentence_tokens, sentence_bigrams\n",
    "\n",
    "tokens2, bigram2 = sentence_to_bigrams('I love language models')\n",
    "\n",
    "print(tokens2)\n",
    "print(bigram2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test2 probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "def bigrams_from_transcript(filename):\n",
    "    \"\"\"\n",
    "    read a file of sentences, adding start '<s>' and stop '</s>' tags; Tokenize it into a list of lower case words\n",
    "    and bigrams\n",
    "    :param filename: string \n",
    "        filename: path to a text file consisting of lines of non-puncuated text; assume one sentence per line\n",
    "    :return: list, list\n",
    "        tokens: ordered list of words found in the file\n",
    "        bigrams: a list of ordered two-word tuples found in the file\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    bigrams = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line_tokens, line_bigrams = sentence_to_bigrams(line)\n",
    "            tokens = tokens + line_tokens\n",
    "            bigrams = bigrams + line_bigrams\n",
    "    return tokens, bigrams\n",
    "\n",
    "\n",
    "def sentence_to_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Add start '<s>' and stop '</s>' tags to the sentence and tokenize it into a list\n",
    "    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)\n",
    "    :param sentence: string\n",
    "    :return: list, list\n",
    "        sentence_tokens: ordered list of words found in the sentence\n",
    "        sentence_bigrams: a list of ordered two-word tuples found in the sentence\n",
    "    \"\"\"\n",
    "    sentence_tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
    "    sentence_bigrams = []\n",
    "    for i in range(len(sentence_tokens)-1):\n",
    "        sentence_bigrams.append((sentence_tokens[i], sentence_tokens[i+1]))\n",
    "    return sentence_tokens, sentence_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0.0, ('<s>', 'go'): 0.034482758620689655, ('go', 'do'): 0.5, ('do', 'you'): 0.625, ('you', 'hear'): 0.058823529411764705, ('hear', '</s>'): 1.0, ('<s>', 'but'): 0.10344827586206896, ('but', 'in'): 0.2, ('in', 'less'): 0.1111111111111111, ('less', 'than'): 1.0, ('than', 'five'): 0.5, ('five', 'minutes'): 1.0, ('minutes', 'the'): 1.0, ('the', 'staircase'): 0.025, ('staircase', 'groaned'): 1.0, ('groaned', 'beneath'): 1.0, ('beneath', 'an'): 1.0, ('an', 'extraordinary'): 0.5, ('extraordinary', 'weight'): 1.0, ('weight', '</s>'): 1.0, ('<s>', 'at'): 0.034482758620689655, ('at', 'this'): 1.0, ('this', 'moment'): 0.16666666666666666, ('moment', 'the'): 1.0, ('the', 'whole'): 0.025, ('whole', 'soul'): 1.0, ('soul', 'of'): 1.0, ('of', 'the'): 0.3, ('the', 'old'): 0.1, ('old', 'man'): 0.75, ('man', 'seemed'): 0.2, ('seemed', 'centred'): 1.0, ('centred', 'in'): 1.0, ('in', 'his'): 0.1111111111111111, ('his', 'eyes'): 0.14285714285714285, ('eyes', 'which'): 0.5, ('which', 'became'): 0.2, ('became', 'bloodshot'): 0.5, ('bloodshot', 'the'): 1.0, ('the', 'veins'): 0.025, ('veins', 'of'): 1.0, ('the', 'throat'): 0.025, ('throat', 'swelled'): 1.0, ('swelled', 'his'): 1.0, ('his', 'cheeks'): 0.2857142857142857, ('cheeks', 'and'): 0.5, ('and', 'temples'): 0.07692307692307693, ('temples', 'became'): 1.0, ('became', 'purple'): 0.5, ('purple', 'as'): 1.0, ('as', 'though'): 0.125, ('though', 'he'): 1.0, ('he', 'was'): 0.25, ('was', 'struck'): 0.14285714285714285, ('struck', 'with'): 1.0, ('with', 'epilepsy'): 0.25, ('epilepsy', 'nothing'): 1.0, ('nothing', 'was'): 1.0, ('was', 'wanting'): 0.14285714285714285, ('wanting', 'to'): 1.0, ('to', 'complete'): 0.07692307692307693, ('complete', 'this'): 1.0, ('this', 'but'): 0.16666666666666666, ('but', 'the'): 0.2, ('the', 'utterance'): 0.025, ('utterance', 'of'): 1.0, ('of', 'a'): 0.2, ('a', 'cry'): 0.2857142857142857, ('cry', '</s>'): 0.3333333333333333, ('<s>', 'and'): 0.034482758620689655, ('and', 'the'): 0.07692307692307693, ('the', 'cry'): 0.025, ('cry', 'issued'): 0.3333333333333333, ('issued', 'from'): 1.0, ('from', 'his'): 1.0, ('his', 'pores'): 0.14285714285714285, ('pores', 'if'): 1.0, ('if', 'we'): 1.0, ('we', 'may'): 0.5, ('may', 'thus'): 1.0, ('thus', 'speak'): 0.5, ('speak', 'a'): 1.0, ('cry', 'frightful'): 0.3333333333333333, ('frightful', 'in'): 1.0, ('in', 'its'): 0.1111111111111111, ('its', 'silence'): 1.0, ('silence', '</s>'): 1.0, ('<s>', 'davrigny'): 0.10344827586206896, ('davrigny', 'rushed'): 0.3333333333333333, ('rushed', 'towards'): 1.0, ('towards', 'the'): 0.5, ('man', 'and'): 0.2, ('and', 'made'): 0.07692307692307693, ('made', 'him'): 0.3333333333333333, ('him', 'inhale'): 0.16666666666666666, ('inhale', 'a'): 1.0, ('a', 'powerful'): 0.14285714285714285, ('powerful', 'restorative'): 1.0, ('restorative', '</s>'): 1.0, ('davrigny', 'unable'): 0.3333333333333333, ('unable', 'to'): 1.0, ('to', 'bear'): 0.07692307692307693, ('bear', 'the'): 1.0, ('the', 'sight'): 0.025, ('sight', 'of'): 1.0, ('of', 'this'): 0.1, ('this', 'touching'): 0.16666666666666666, ('touching', 'emotion'): 1.0, ('emotion', 'turned'): 1.0, ('turned', 'away'): 1.0, ('away', 'and'): 1.0, ('and', 'villefort'): 0.07692307692307693, ('villefort', 'without'): 0.3333333333333333, ('without', 'seeking'): 1.0, ('seeking', 'any'): 1.0, ('any', 'further'): 1.0, ('further', 'explanation'): 1.0, ('explanation', 'and'): 1.0, ('and', 'attracted'): 0.07692307692307693, ('attracted', 'towards'): 1.0, ('towards', 'him'): 0.25, ('him', 'by'): 0.16666666666666666, ('by', 'the'): 0.5, ('the', 'irresistible'): 0.025, ('irresistible', 'magnetism'): 1.0, ('magnetism', 'which'): 1.0, ('which', 'draws'): 0.2, ('draws', 'us'): 1.0, ('us', 'towards'): 1.0, ('towards', 'those'): 0.25, ('those', 'who'): 0.5, ('who', 'have'): 0.5, ('have', 'loved'): 1.0, ('loved', 'the'): 1.0, ('the', 'people'): 0.025, ('people', 'for'): 1.0, ('for', 'whom'): 0.3333333333333333, ('whom', 'we'): 1.0, ('we', 'mourn'): 0.5, ('mourn', 'extended'): 1.0, ('extended', 'his'): 1.0, ('his', 'hand'): 0.14285714285714285, ('hand', 'towards'): 1.0, ('the', 'young'): 0.025, ('young', 'man'): 1.0, ('man', '</s>'): 0.2, ('<s>', 'for'): 0.034482758620689655, ('for', 'some'): 0.3333333333333333, ('some', 'time'): 1.0, ('time', 'nothing'): 0.5, ('was', 'heard'): 0.14285714285714285, ('heard', 'in'): 1.0, ('in', 'that'): 0.1111111111111111, ('that', 'chamber'): 0.25, ('chamber', 'but'): 0.5, ('but', 'sobs'): 0.2, ('sobs', 'exclamations'): 1.0, ('exclamations', 'and'): 1.0, ('and', 'prayers'): 0.07692307692307693, ('prayers', '</s>'): 0.5, ('<s>', 'what'): 0.034482758620689655, ('what', 'do'): 1.0, ('you', 'mean'): 0.058823529411764705, ('mean', 'sir'): 1.0, ('sir', '</s>'): 0.3333333333333333, ('<s>', 'oh'): 0.034482758620689655, ('oh', 'you'): 1.0, ('you', 'rave'): 0.058823529411764705, ('rave', 'sir'): 0.5, ('sir', 'exclaimed'): 0.3333333333333333, ('exclaimed', 'villefort'): 1.0, ('villefort', 'in'): 0.3333333333333333, ('in', 'vain'): 0.1111111111111111, ('vain', 'endeavoring'): 1.0, ('endeavoring', 'to'): 1.0, ('to', 'escape'): 0.15384615384615385, ('escape', 'the'): 0.5, ('the', 'net'): 0.025, ('net', 'in'): 1.0, ('in', 'which'): 0.1111111111111111, ('which', 'he'): 0.2, ('was', 'taken'): 0.14285714285714285, ('taken', 'i'): 1.0, ('i', 'rave'): 0.1111111111111111, ('rave', '</s>'): 0.5, ('<s>', 'do'): 0.034482758620689655, ('you', 'know'): 0.058823529411764705, ('know', 'the'): 1.0, ('the', 'assassin'): 0.025, ('assassin', 'asked'): 1.0, ('asked', 'morrel'): 1.0, ('morrel', '</s>'): 0.2, ('<s>', 'noirtier'): 0.06896551724137931, ('noirtier', 'looked'): 0.25, ('looked', 'upon'): 1.0, ('upon', 'morrel'): 1.0, ('morrel', 'with'): 0.2, ('with', 'one'): 0.25, ('one', 'of'): 1.0, ('of', 'those'): 0.1, ('those', 'melancholy'): 0.5, ('melancholy', 'smiles'): 1.0, ('smiles', 'which'): 1.0, ('which', 'had'): 0.2, ('had', 'so'): 0.5, ('so', 'often'): 0.3333333333333333, ('often', 'made'): 1.0, ('made', 'valentine'): 0.3333333333333333, ('valentine', 'happy'): 1.0, ('happy', 'and'): 1.0, ('and', 'thus'): 0.07692307692307693, ('thus', 'fixed'): 0.5, ('fixed', 'his'): 0.5, ('his', 'attention'): 0.14285714285714285, ('attention', '</s>'): 1.0, ('<s>', 'said'): 0.034482758620689655, ('said', 'morrel'): 0.25, ('morrel', 'sadly'): 0.2, ('sadly', 'yes'): 1.0, ('yes', 'replied'): 0.3333333333333333, ('replied', 'noirtier'): 1.0, ('noirtier', '</s>'): 0.25, ('<s>', 'the'): 0.1724137931034483, ('old', 'mans'): 0.25, ('mans', 'eyes'): 1.0, ('eyes', 'remained'): 0.5, ('remained', 'fixed'): 1.0, ('fixed', 'on'): 0.5, ('on', 'the'): 0.6666666666666666, ('the', 'door'): 0.05, ('door', '</s>'): 0.3333333333333333, ('<s>', 'asked'): 0.034482758620689655, ('morrel', 'yes'): 0.2, ('yes', '</s>'): 0.6666666666666666, ('<s>', 'must'): 0.034482758620689655, ('must', 'i'): 1.0, ('i', 'leave'): 0.1111111111111111, ('leave', 'alone'): 1.0, ('alone', 'no'): 0.3333333333333333, ('no', '</s>'): 0.5, ('but', 'can'): 0.2, ('can', 'he'): 0.5, ('he', 'understand'): 0.125, ('understand', 'you'): 1.0, ('you', 'yes'): 0.058823529411764705, ('<s>', 'gentlemen'): 0.034482758620689655, ('gentlemen', 'he'): 1.0, ('he', 'said'): 0.125, ('said', 'in'): 0.25, ('in', 'a'): 0.1111111111111111, ('a', 'hoarse'): 0.14285714285714285, ('hoarse', 'voice'): 1.0, ('voice', 'give'): 1.0, ('give', 'me'): 1.0, ('me', 'your'): 0.3333333333333333, ('your', 'word'): 1.0, ('word', 'of'): 1.0, ('of', 'honor'): 0.1, ('honor', 'that'): 1.0, ('that', 'this'): 0.25, ('this', 'horrible'): 0.16666666666666666, ('horrible', 'secret'): 1.0, ('secret', 'shall'): 0.5, ('shall', 'forever'): 0.5, ('forever', 'remain'): 1.0, ('remain', 'buried'): 1.0, ('buried', 'amongst'): 1.0, ('amongst', 'ourselves'): 0.5, ('ourselves', 'the'): 1.0, ('the', 'two'): 0.05, ('two', 'men'): 0.5, ('men', 'drew'): 1.0, ('drew', 'back'): 1.0, ('back', '</s>'): 1.0, ('<s>', 'my'): 0.034482758620689655, ('my', 'father'): 0.6666666666666666, ('father', 'has'): 0.3333333333333333, ('has', 'revealed'): 1.0, ('revealed', 'the'): 1.0, ('the', 'culprits'): 0.025, ('culprits', 'name'): 1.0, ('name', 'my'): 1.0, ('father', 'thirsts'): 0.3333333333333333, ('thirsts', 'for'): 1.0, ('for', 'revenge'): 0.3333333333333333, ('revenge', 'as'): 1.0, ('as', 'much'): 0.125, ('much', 'as'): 1.0, ('as', 'you'): 0.25, ('you', 'do'): 0.058823529411764705, ('do', 'yet'): 0.125, ('yet', 'even'): 1.0, ('even', 'he'): 1.0, ('he', 'conjures'): 0.125, ('conjures', 'you'): 1.0, ('you', 'as'): 0.11764705882352941, ('as', 'i'): 0.25, ('i', 'do'): 0.2222222222222222, ('do', 'to'): 0.125, ('to', 'keep'): 0.07692307692307693, ('keep', 'this'): 1.0, ('this', 'secret'): 0.16666666666666666, ('secret', 'do'): 0.5, ('you', 'not'): 0.11764705882352941, ('not', 'father'): 0.25, ('father', '</s>'): 0.3333333333333333, ('<s>', 'morrel'): 0.034482758620689655, ('morrel', 'suffered'): 0.2, ('suffered', 'an'): 1.0, ('an', 'exclamation'): 0.5, ('exclamation', 'of'): 1.0, ('of', 'horror'): 0.1, ('horror', 'and'): 1.0, ('and', 'surprise'): 0.07692307692307693, ('surprise', 'to'): 1.0, ('escape', 'him'): 0.5, ('him', '</s>'): 0.3333333333333333, ('man', 'made'): 0.2, ('made', 'a'): 0.3333333333333333, ('a', 'sign'): 0.14285714285714285, ('sign', 'in'): 1.0, ('in', 'the'): 0.1111111111111111, ('the', 'affirmative'): 0.025, ('affirmative', '</s>'): 1.0, ('<s>', 'it'): 0.034482758620689655, ('it', 'was'): 1.0, ('was', 'something'): 0.14285714285714285, ('something', 'terrible'): 1.0, ('terrible', 'to'): 1.0, ('to', 'witness'): 0.07692307692307693, ('witness', 'the'): 1.0, ('the', 'silent'): 0.025, ('silent', 'agony'): 0.5, ('agony', 'the'): 1.0, ('the', 'mute'): 0.025, ('mute', 'despair'): 1.0, ('despair', 'of'): 1.0, ('of', 'noirtier'): 0.1, ('noirtier', 'whose'): 0.25, ('whose', 'tears'): 1.0, ('tears', 'silently'): 1.0, ('silently', 'rolled'): 1.0, ('rolled', 'down'): 1.0, ('down', 'his'): 1.0, ('cheeks', '</s>'): 0.5, ('but', 'he'): 0.2, ('he', 'stopped'): 0.125, ('stopped', 'on'): 1.0, ('the', 'landing'): 0.025, ('landing', 'he'): 1.0, ('he', 'had'): 0.125, ('had', 'not'): 0.5, ('not', 'the'): 0.25, ('the', 'courage'): 0.025, ('courage', 'to'): 1.0, ('to', 'again'): 0.07692307692307693, ('again', 'visit'): 1.0, ('visit', 'the'): 1.0, ('the', 'death'): 0.025, ('death', 'chamber'): 1.0, ('chamber', '</s>'): 0.5, ('two', 'doctors'): 0.5, ('doctors', 'therefore'): 1.0, ('therefore', 'entered'): 1.0, ('entered', 'the'): 1.0, ('the', 'room'): 0.025, ('room', 'alone'): 0.5, ('alone', '</s>'): 0.3333333333333333, ('noirtier', 'was'): 0.25, ('was', 'near'): 0.14285714285714285, ('near', 'the'): 1.0, ('the', 'bed'): 0.025, ('bed', 'pale'): 1.0, ('pale', 'motionless'): 1.0, ('motionless', 'and'): 1.0, ('and', 'silent'): 0.07692307692307693, ('silent', 'as'): 0.5, ('as', 'the'): 0.125, ('the', 'corpse'): 0.025, ('corpse', '</s>'): 1.0, ('the', 'district'): 0.05, ('district', 'doctor'): 1.0, ('doctor', 'approached'): 0.5, ('approached', 'with'): 1.0, ('with', 'the'): 0.25, ('the', 'indifference'): 0.025, ('indifference', 'of'): 1.0, ('a', 'man'): 0.14285714285714285, ('man', 'accustomed'): 0.2, ('accustomed', 'to'): 1.0, ('to', 'spend'): 0.07692307692307693, ('spend', 'half'): 1.0, ('half', 'his'): 1.0, ('his', 'time'): 0.14285714285714285, ('time', 'amongst'): 0.5, ('amongst', 'the'): 0.5, ('the', 'dead'): 0.025, ('dead', 'he'): 1.0, ('he', 'then'): 0.125, ('then', 'lifted'): 1.0, ('lifted', 'the'): 1.0, ('the', 'sheet'): 0.025, ('sheet', 'which'): 1.0, ('which', 'was'): 0.2, ('was', 'placed'): 0.14285714285714285, ('placed', 'over'): 1.0, ('over', 'the'): 1.0, ('the', 'face'): 0.025, ('face', 'and'): 1.0, ('and', 'just'): 0.07692307692307693, ('just', 'unclosed'): 1.0, ('unclosed', 'the'): 1.0, ('the', 'lips'): 0.025, ('lips', '</s>'): 1.0, ('the', 'nearest'): 0.025, ('nearest', 'said'): 1.0, ('said', 'the'): 0.25, ('doctor', 'is'): 0.5, ('is', 'a'): 0.5, ('a', 'good'): 0.14285714285714285, ('good', 'italian'): 1.0, ('italian', 'abbe'): 1.0, ('abbe', 'who'): 1.0, ('who', 'lives'): 0.5, ('lives', 'next'): 1.0, ('next', 'door'): 1.0, ('door', 'to'): 0.3333333333333333, ('to', 'you'): 0.07692307692307693, ('you', 'shall'): 0.058823529411764705, ('shall', 'i'): 0.5, ('i', 'call'): 0.1111111111111111, ('call', 'on'): 1.0, ('on', 'him'): 0.3333333333333333, ('him', 'as'): 0.16666666666666666, ('i', 'pass'): 0.1111111111111111, ('pass', '</s>'): 1.0, ('davrigny', 'said'): 0.3333333333333333, ('said', 'villefort'): 0.25, ('villefort', 'be'): 0.3333333333333333, ('be', 'so'): 0.3333333333333333, ('so', 'kind'): 0.3333333333333333, ('kind', 'i'): 1.0, ('i', 'beseech'): 0.1111111111111111, ('beseech', 'you'): 1.0, ('as', 'to'): 0.125, ('to', 'accompany'): 0.07692307692307693, ('accompany', 'this'): 1.0, ('this', 'gentleman'): 0.16666666666666666, ('gentleman', 'here'): 1.0, ('here', 'is'): 1.0, ('is', 'the'): 0.5, ('the', 'key'): 0.025, ('key', 'of'): 1.0, ('door', 'so'): 0.3333333333333333, ('so', 'that'): 0.3333333333333333, ('that', 'you'): 0.25, ('you', 'can'): 0.058823529411764705, ('can', 'go'): 0.5, ('go', 'in'): 0.5, ('in', 'and'): 0.1111111111111111, ('and', 'out'): 0.07692307692307693, ('out', 'as'): 1.0, ('you', 'please'): 0.058823529411764705, ('please', 'you'): 1.0, ('you', 'will'): 0.11764705882352941, ('will', 'bring'): 0.2, ('bring', 'the'): 1.0, ('the', 'priest'): 0.025, ('priest', 'with'): 1.0, ('with', 'you'): 0.25, ('you', 'and'): 0.058823529411764705, ('and', 'will'): 0.07692307692307693, ('will', 'oblige'): 0.2, ('oblige', 'me'): 1.0, ('me', 'by'): 0.3333333333333333, ('by', 'introducing'): 0.5, ('introducing', 'him'): 1.0, ('him', 'into'): 0.16666666666666666, ('into', 'my'): 1.0, ('my', 'childs'): 0.3333333333333333, ('childs', 'room'): 1.0, ('room', 'do'): 0.5, ('you', 'wish'): 0.058823529411764705, ('wish', 'to'): 1.0, ('to', 'see'): 0.07692307692307693, ('see', 'him'): 1.0, ('<s>', 'i'): 0.06896551724137931, ('i', 'only'): 0.1111111111111111, ('only', 'wish'): 1.0, ('to', 'be'): 0.07692307692307693, ('be', 'alone'): 0.3333333333333333, ('alone', 'you'): 0.3333333333333333, ('will', 'excuse'): 0.2, ('excuse', 'me'): 1.0, ('me', 'will'): 0.3333333333333333, ('will', 'you'): 0.2, ('not', '</s>'): 0.25, ('i', 'am'): 0.1111111111111111, ('am', 'going'): 1.0, ('going', 'sir'): 1.0, ('sir', 'and'): 0.3333333333333333, ('and', 'i'): 0.07692307692307693, ('do', 'not'): 0.125, ('not', 'hesitate'): 0.25, ('hesitate', 'to'): 1.0, ('to', 'say'): 0.07692307692307693, ('say', 'that'): 1.0, ('that', 'no'): 0.25, ('no', 'prayers'): 0.5, ('prayers', 'will'): 0.5, ('will', 'be'): 0.2, ('be', 'more'): 0.3333333333333333, ('more', 'fervent'): 1.0, ('fervent', 'than'): 1.0, ('than', 'mine'): 0.5, ('mine', '</s>'): 1.0}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def sample_run():\n",
    "    # sample usage by test code (this definition not actually run for the quiz)\n",
    "    tokens, bigrams = bigrams_from_transcript('transcripts.txt')\n",
    "    bg_dict = bigram_mle(tokens, bigrams)\n",
    "    print(bg_dict)\n",
    "\n",
    "\n",
    "def bigram_mle(tokens, bigrams):\n",
    "    \"\"\"\n",
    "    provide a dictionary of probabilities for all bigrams in a corpus of text\n",
    "    the calculation is based on maximum likelihood estimation and does not include\n",
    "    any smoothing.  A tag '<unk>' has been added for unknown probabilities.\n",
    "    :param tokens: list\n",
    "        tokens: list of all tokens in the corpus\n",
    "    :param bigrams: list\n",
    "        bigrams: list of all two word tuples in the corpus\n",
    "    :return: dict\n",
    "        bg_mle_dict: a dictionary of bigrams:\n",
    "            key: tuple of two bigram words, in order OR <unk> key\n",
    "            value: float probability\n",
    "\n",
    "    \"\"\"\n",
    "    bg_mle_dict = {}\n",
    "    bg_mle_dict['<unk>'] = .0\n",
    "    #TODO implement\n",
    "    token_raw = Counter(tokens)\n",
    "    bigram_raw = Counter(bigrams)\n",
    "    for bg in bigram_raw:\n",
    "        bg_mle_dict[bg] = bigram_raw[bg] / token_raw[bg[0]]\n",
    "    return bg_mle_dict\n",
    "\n",
    "sample_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test3  Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def bigrams_from_transcript(filename):\n",
    "    \"\"\"\n",
    "    read a file of sentences, adding start '<s>' and stop '</s>' tags; Tokenize it into a list of lower case words\n",
    "    and bigrams\n",
    "    :param filename: string \n",
    "        filename: path to a text file consisting of lines of non-puncuated text; assume one sentence per line\n",
    "    :return: list, list\n",
    "        tokens: ordered list of words found in the file\n",
    "        bigrams: a list of ordered two-word tuples found in the file\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    bigrams = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line_tokens, line_bigrams = sentence_to_bigrams(line)\n",
    "            tokens = tokens + line_tokens\n",
    "            bigrams = bigrams + line_bigrams\n",
    "    return tokens, bigrams\n",
    "\n",
    "\n",
    "def sentence_to_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Add start '<s>' and stop '</s>' tags to the sentence and tokenize it into a list\n",
    "    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)\n",
    "    :param sentence: string\n",
    "    :return: list, list\n",
    "        sentence_tokens: ordered list of words found in the sentence\n",
    "        sentence_bigrams: a list of ordered two-word tuples found in the sentence\n",
    "    \"\"\"\n",
    "    sentence_tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
    "    sentence_bigrams = []\n",
    "    for i in range(len(sentence_tokens)-1):\n",
    "        sentence_bigrams.append((sentence_tokens[i], sentence_tokens[i+1]))\n",
    "    return sentence_tokens, sentence_bigrams\n",
    "\n",
    "def bigram_add1_logs(transcript_file):\n",
    "    \"\"\"\n",
    "    provide a smoothed log probability dictionary based on a transcript\n",
    "    :param transcript_file: string\n",
    "        transcript_file is the path filename containing unpunctuated text sentences\n",
    "    :return: dict\n",
    "        bg_add1_log_dict: dictionary of smoothed bigrams log probabilities including\n",
    "        tags: <s>: start of sentence, </s>: end of sentence, <unk>: unknown placeholder probability\n",
    "    \"\"\"\n",
    "\n",
    "    tokens, bigrams = bigrams_from_transcript(transcript_file)\n",
    "    token_counts = Counter(tokens)\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    vocab_count = len(token_counts)\n",
    "\n",
    "    bg_addone_dict = {}\n",
    "    for bg in bigram_counts:\n",
    "        bg_addone_dict[bg] = np.log((bigram_counts[bg] + 1.) / (token_counts[bg[0]] + vocab_count))\n",
    "    bg_addone_dict['<unk>'] = np.log(1. / vocab_count)\n",
    "    return bg_addone_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** \"the old man spoke to me\"\n",
      "-34.80495531345013\n",
      "*** \"me to spoke man old the\"\n",
      "-39.34280606002005\n",
      "*** \"old man me old man me\"\n",
      "-36.59899481268447\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'the old man spoke to me',\n",
    "    'me to spoke man old the',\n",
    "    'old man me old man me',\n",
    "]\n",
    "\n",
    "def sample_run():\n",
    "    # sample usage by test code (this definition not actually run for the quiz)\n",
    "    bigram_log_dict = bigram_add1_logs('transcripts.txt')\n",
    "    for sentence in test_sentences:\n",
    "        print('*** \"{}\"'.format(sentence))\n",
    "        print(log_prob_of_sentence(sentence, bigram_log_dict))\n",
    "\n",
    "def log_prob_of_sentence(sentence, bigram_log_dict):\n",
    "#     total_log_prob = 0.\n",
    "\n",
    "    # TODO implement\n",
    "    # get the sentence bigrams with utils.sentence_to_bigrams\n",
    "    # look up the bigrams from the sentence in the bigram_log_dict\n",
    "    # add all the the log probabilities together\n",
    "    # if a word doesn't exist, be sure to use the value of the '<unk>' lookup instead\n",
    "\n",
    "    # get the sentence bigrams\n",
    "    s_tokens, s_bigrams = sentence_to_bigrams(sentence)\n",
    "\n",
    "    # add the log probabilites of the bigrams in the sentence\n",
    "    total_log_prob = 0.\n",
    "    for bg in s_bigrams:\n",
    "        if bg in bigram_log_dict:\n",
    "            total_log_prob = total_log_prob + bigram_log_dict[bg]\n",
    "        else:\n",
    "            total_log_prob = total_log_prob + bigram_log_dict['<unk>']\n",
    "    return total_log_prob\n",
    "\n",
    "sample_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
