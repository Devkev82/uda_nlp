{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'As I was waiting, a man came out of a side room, and at a glance I was sure he must be Long John. \\\n",
    "His left leg was cut off close by the hip, and under the left shoulder he carried a crutch, which he managed \\\n",
    "with wonderful dexterity, hopping about upon it like a bird. He was very tall and strong, with a face as big \\\n",
    "as a ham—plain and pale, but intelligent and smiling. Indeed, he seemed in the most cheerful spirits, whistling \\\n",
    "as he moved about among the tables, with a merry word or a slap on the shoulder for the more favoured of his guests.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_low = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as i was waiting, a man came out of a side room, and at a glance i was sure he must be long john. his left leg was cut off close by the hip, and under the left shoulder he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird. he was very tall and strong, with a face as big as a ham—plain and pale, but intelligent and smiling. indeed, he seemed in the most cheerful spirits, whistling as he moved about among the tables, with a merry word or a slap on the shoulder for the more favoured of his guests.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "s = re.compile('[a-zA-Z]*')\n",
    "l = [i for i in s.findall(text_low) if len(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3,3,2,4]\n",
    "d = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function get:\n",
      "\n",
      "get(key, default=None, /) method of builtins.dict instance\n",
      "    Return the value for key if key is in the dictionary, else default.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(d.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in l:\n",
    "    d[n] = d.get(n, 0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 2, 3: 2, 4: 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getUrlSpecifyText(url='https://cn.udacity.com/courses/all'):\n",
    "    l = []\n",
    "    \n",
    "    req = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    \n",
    "    summaries = soup.find_all('div', class_='course-summary-card')\n",
    "    for tag in summaries:\n",
    "        l_sub = []\n",
    "        \n",
    "        l_sub.append(tag.select_one('h3 a').get_text().strip())          # 课程名称\n",
    "        \n",
    "        r = tag.find_all('span', 'ng-star-inserted')                     # 课程等级\n",
    "        l_sub.append(r[0].get_text().strip())\n",
    "        try:\n",
    "            l_sub.append(r[1].get_text().strip())                        # 课程简介\n",
    "        except IndexError as e:\n",
    "            l_sub.append('None')                                         # 简介为空，则设为‘None’\n",
    "            continue\n",
    "             \n",
    "        l.append(l_sub)\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AI 产品经理 (英)', '初级', '驱动 AI 产品商业化落地，用 AI 为业务赋能，成为炙手可热稀缺人才。'],\n",
       " ['数据挖掘 求职直通班',\n",
       "  '中级',\n",
       "  '从数据分析、机器学习开始，掌握推荐系统和大数据计算框架 Spark 等核心技能，通过学习、项目、拓展、案例和求职辅导，系统完善对接职场需求，一站式直达职业目标'],\n",
       " ['市场营销分析 (英)',\n",
       "  '初级',\n",
       "  '掌握市场营销所需的基本数据技能，使用 Google Analytics 和 Data Studio 呈现你的结论'],\n",
       " ['传感器融合 (英)', '高级', '和梅赛德斯-奔驰学习，成为掌握无人驾驶、物联网、机器人开发核心技术的抢手工程师。'],\n",
       " ['数据洞察与说服技巧 (英)', '中级', '掌握数据可视化呈现商业洞察，练就用数据影响决策的说服技巧。'],\n",
       " ['云计算软件开发 (英)',\n",
       "  '中级',\n",
       "  '学习在 AWS 上构建全栈应用，并使用 Kubernetes 和 Serverless 框架分发采用微服务的应用，成为炙手可热的云计算软件开发工程师。'],\n",
       " ['云计算 DevOps (英)',\n",
       "  '中级',\n",
       "  '学习在 AWS 上通过代码部署基础设施和应用，构建 CI/CD 管道，以及使用 Kubernetes 和其他现代工具运维化微服务，成为一名抢手的云计算 DevOps 工程师。'],\n",
       " ['AI 求职直通班',\n",
       "  '中级',\n",
       "  '一站掌握机器学习、深度学习、CV、NLP 等 AI 核心技能，覆盖金融风控、计算机视觉、自然语言处理、数据挖掘、计算广告、语音识别六大热门领域，高性价比对接职场需求'],\n",
       " ['C++ 程序设计 (英)', '中级', '全面提升面向对象编程能力，成为 C++ 程序设计的专家。'],\n",
       " ['数据结构与算法 (英)', '中级', '精选 100+ 道面试真题，挑战数据结构与算法习题，获得详细项目审阅反馈，轻松应对求职面试和工作挑战。']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getUrlSpecifyText()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. \n",
    "By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, \n",
    "named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.\n",
    "“Apart from common word processor operations that treat text like a mere sequence of symbols, NLP considers the hierarchical \n",
    "structure of language: several words make a phrase, several phrases make a sentence and, ultimately, sentences convey ideas,” \n",
    "John Rehling, an NLP expert at Meltwater Group, said in How Natural Language Processing Helps Uncover Social Media Sentiment. \n",
    "“By analyzing language for its meaning, NLP systems have long filled useful roles, such as correcting grammar, converting speech \n",
    "to text and automatically translating between languages.”\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def getNormalizedText(text=s):\n",
    "    s_c = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    \n",
    "    # tokenize\n",
    "    words = word_tokenize(s_c)\n",
    "    \n",
    "    # stopwords\n",
    "    w_s = [w for w in words if w not in stopwords.words('english')]\n",
    "    \n",
    "    return w_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP',\n",
       " 'way',\n",
       " 'computers',\n",
       " 'analyze',\n",
       " 'understand',\n",
       " 'derive',\n",
       " 'meaning',\n",
       " 'human',\n",
       " 'language',\n",
       " 'smart']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getNormalizedText()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句子解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#  nltk.ChartParser(my_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define a custom grammar\n",
    "my_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(my_grammar)\n",
    "\n",
    "# Parse a sentence\n",
    "sentence = word_tokenize(\"I shot an elephant in my pajamas\")\n",
    "for tree in parser.parse(sentence):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e613bbfcd985>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Reduce words to their stems\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mstemmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemmed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-973660e6e5fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Reduce words to their root form\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlemmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize verbs by specifying pos\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\n",
    "print(lemmed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
